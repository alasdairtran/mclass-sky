{
 "metadata": {
  "name": "",
  "signature": "sha256:2a5b0367f3c48e94fcff2f36b54c9dd6efe04b7ce35e94b54123d4067f879cd8"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Performance Measures"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.core.display import HTML\n",
      "HTML(open(\"styles/stylesheet.css\", \"r\").read())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<link href='<link href='http://fonts.googleapis.com/css?family=Source+Code+Pro:400,700|Droid+Serif:400,700,400italic|Open+Sans:400italic,700italic,400,700' rel='stylesheet' type='text/css'>\n",
        "\n",
        "<style>\n",
        "\tbody {\n",
        "\t\tfont-family: 'Open Sans', sans-serif;\n",
        "\t}\n",
        "    \n",
        "    \n",
        "    /* limit cell width\n",
        "\tdiv.cell { \n",
        "\t\tmax-width: 1000px;\n",
        "\t\tmargin-left: auto;\n",
        "\t\tmargin-right: auto;\n",
        "\t} */\n",
        "    \n",
        "    /* More space between Markdown bullet points */\n",
        "\t#notebook li { \n",
        "\t\tmargin-top: 0.5em;\n",
        "\t} \n",
        "\n",
        "\t/* draw border around running cells */\n",
        "\tdiv.cell.border-box-sizing.code_cell.running { \n",
        "\t\tborder-width: thin;\n",
        "\t\tborder-style: solid;\n",
        "\t\tborder-color: red;\n",
        "\t\tborder-radius: 10px;\n",
        "\t}\n",
        "\n",
        "\t/* Put a solid color box around each cell and its output, visually linking\n",
        "    them together */\n",
        "\tdiv.cell.code_cell {\n",
        "\t\tborder-radius: 10px; /* rounded borders */\n",
        "\t\tpadding: 1em;\n",
        "\t}\n",
        "\n",
        "\t/* Markdown cells */\n",
        "\tdiv.text_cell_render {\n",
        "        background: #fff;\n",
        "        color: #444;\n",
        "        font-family: 'Open Sans', sans-serif;\n",
        "        line-height: 1.75;\n",
        "\t}\n",
        "    \n",
        "\t/* Formatting code in markdown cells */\n",
        "\tdiv.rendered_html code {\n",
        "\t\tfont-family: 'Source Code Pro', monospace;\n",
        "        line-height: 1.5;\n",
        "\t}\n",
        "    \n",
        "\t/* Formatting for header cells */\n",
        "    .text_cell_render h1, .text_cell_render h2, .text_cell_render h3,\n",
        "    .text_cell_render h4, .text_cell_render h5, .text_cell_render h6 {\n",
        "        font-family: 'Droid Serif', serif;\n",
        "        display: block;\n",
        "        font-weight: normal;\n",
        "    }\n",
        "    \n",
        "\t.text_cell_render h1 {\n",
        "\t\tfont-size: 250%;\n",
        "        text-align: center;\n",
        "\t\tcolor: #c65c3f;\n",
        "\t}\n",
        "    \n",
        "\t.text_cell_render h2 {\n",
        "        font-size: 200%;\n",
        "        color: #c65c3f;\n",
        "\t}\t\n",
        "\n",
        "\t.text_cell_render h3 {\n",
        "        font-size: 180%;\n",
        "        color: rgb(12,85,97);\n",
        "        font-weight: normal;\n",
        "\t}\n",
        "\n",
        "\t.text_cell_render h4 {\n",
        "        font-size: 150%;\n",
        "        color: rgb(12,85,97);\n",
        "\t}\n",
        "\n",
        "\t.text_cell_render h5 {\n",
        "        font-size: 110%;\n",
        "        color: #b2b888;\n",
        "        text-transform: uppercase;\n",
        "\t}\n",
        "\n",
        "\t.text_cell_render h6 {\n",
        "        font-size: 100%;\n",
        "\t\tcolor: #b2b888;\n",
        "        text-transform: uppercase;\n",
        "\t}\n",
        "\n",
        "\t.CodeMirror{\n",
        "\t\tfont-family: 'Inconsolata', monospace;\n",
        "        line-height: 1.5;\n",
        "\t}\n",
        "\n",
        "\t/* Hide the prompts, e.g. In[1], Out[1] */\n",
        "    .prompt{\n",
        "        display: None;\n",
        "    }\n",
        "\t\n",
        "\t/* Usage: <div class=warning> message here </div> */\n",
        "\t.warning{\n",
        "\t\tbackground-color: #fcf2f2;\n",
        "\t\tborder-color: #dFb5b4;\n",
        "\t\tborder-left: 5px solid #dfb5b4;\n",
        "\t\tpadding: 0.5em;\n",
        "\t}\n",
        "</style>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 34,
       "text": [
        "<IPython.core.display.HTML at 0x59d52b0>"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This notebook explains the script <a href=\"scripts/classifier_performance.py\" target=\"_blank\"> `scripts/performance_measures.py`</a>, which contains various functions that evaluate the performance of a classifier."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Some notations:\n",
      "\n",
      "* $C$ is the confusion matrix, where entry $C_{ij}$ is the number of objects in class $i$ but have been as class $j$,\n",
      "* $m_i$ is the number of objects in class $i$: $m_i = \\sum_j C_{ij}$,\n",
      "* $m$ is the total sample size: $m = \\sum_i m_i$,\n",
      "* $k$ is the number of classes."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "1. Naive Accuracy"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One simple way to evaluate the overall performance of a classifier is to compute the naive accuracy rate, which is simply the total fraction of objects that have been correctly classified:\n",
      "\\begin{aligned}\n",
      "    \\text{Naive Accuracy Rate} = \\dfrac{\\sum_i C_{ii}}{m}\n",
      "\\end{aligned}\n",
      "\n",
      "This is implemented in the function **`naive_accuracy()`**:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load -s naive_accuracy scripts/classifier_performance.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def naive_accuracy(confusions, classifiers):\n",
      "    \"\"\"\n",
      "    Input: confusion matrix and array of classifier names (column index)\n",
      "    \"\"\"\n",
      "    \n",
      "    results_dict = {}\n",
      "    \n",
      "    for clf, conf in confusions.items():\n",
      "        results_dict[clf] = np.trace(conf) / np.sum(conf)\n",
      "\n",
      "    results_df = DataFrame(results_dict, index=[\"Accuray\"])\n",
      "    results_df = results_df.reindex(columns=classifiers, copy=False)\n",
      "    \n",
      "    return results_df\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "2. Balanced Accuracy"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "However, if the dataset is imbalanced, this measure would not work well. A better approach is to use the <a href=\"http://ong-home.my/papers/brodersen10post-balacc.pdf\" target=\"_blank\">posterior balanced accuracy</a>. Let $A_i$ be the accuracy rate of class $i$:\n",
      "\\begin{aligned}\n",
      "    A_i = \\dfrac{C_{ii}}{m_i}\n",
      "\\end{aligned}\n",
      "\n",
      "Before running a classifier, we know nothing of its performance, so we can assume the accuracy rate follows a flat prior distribution. In particular, the Beta distribution with parameters $\\alpha = \\beta = 1$ (i.e. a uniform distriubtion) seems appropriate here:\n",
      "\\begin{aligned}\n",
      "    A_i \\sim Beta(1, 1) \\qquad \\forall i\n",
      "\\end{aligned}\n",
      "\n",
      "Given an accuracy rate $A_i$ for each class $i$, the number of correct predictions in class $i$ will follow a Binomial distribution with $A_i$ as the probability of success:\n",
      "\\begin{aligned}\n",
      "    \\big( C_{ii} \\mid A_i \\big) \\sim Bin\\big(m_i, A_i\\big)  \\qquad \\forall i\n",
      "\\end{aligned}\n",
      "\n",
      "In Bayesian terms, this is our likelihood. Now we know that with respect to a Binomial likelihood, the Beta distribution is conjugate to itself. Thus the posterior distribution of $A_i$ will also be Beta with parameters:\n",
      "\\begin{aligned}\n",
      "    \\big( A_i \\mid C_{ii} \\big) \\sim Beta \\big( 1 + C_{ii}, 1 + m_i - C_{ii} \\big) \\qquad \\forall i\n",
      "\\end{aligned}\n",
      "\n",
      "One way to define the balanced accuracy $A$ is to take the average of the individual accuracy rates $A_i$:\n",
      "\\begin{aligned}\n",
      "    A = \\dfrac{1}{k} \\sum_i A_i\n",
      "\\end{aligned}\n",
      "\n",
      "We call $\\big( A \\mid C \\big)$ the posterior balanced accuracy. One nice feature of this measure is that it's a probability distribution (instead of a simple point estimate). This allows us to construct confidence intervals, etc. And even though there is no closed form solution for the density function of $\\big( A \\mid C \\big)$, we can still compute it by performing the <a href=\"https://en.wikipedia.org/wiki/Probability_density_function#Sums_of_independent_random_variables\" target=\"_blank\">convolution</a> $k$ times. This can easily be done in Python. Here's an implementation."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load -s convolve_betas scripts/classifier_performance.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def convolve_betas(parameters, res=0.001):\n",
      "    \"\"\" Convolves k Beta distributions. Parameters is a list of tuples (alpha_i, beta_i)\"\"\"\n",
      "    \n",
      "    # number of convolution\n",
      "    k = len(parameters)\n",
      "    \n",
      "    # sum of three probabilities ranges from 0 to k\n",
      "    x = np.arange(0, k+res, res)\n",
      "    \n",
      "    # compute the individual beta pdfs\n",
      "    pdfs = []\n",
      "    for par in parameters:\n",
      "        alpha = par[0]\n",
      "        beta = par[1]\n",
      "        pdfs.append(beta.pdf(x, alpha, beta))\n",
      "        \n",
      "    # convolve k times\n",
      "    convolution = pdfs[0]\n",
      "    for i in range(1, k):\n",
      "        convolution = np.convolve(convolution, pdfs[i])\n",
      "        \n",
      "    # reduce to the [0, k] support\n",
      "    convolution = convolution[0:len(x)]\n",
      "    \n",
      "    # normalise so that all values sum to (1 / res)\n",
      "    convolution = convolution / (sum(convolution) * res)\n",
      "    \n",
      "    return convolution\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We have just approximated the density of the sum of $k$ Beta distributions. The next step is to present our results. One measure we can report is the expected value of the posterior balanced accuracy."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load -s balanced_accuracy_expected scripts/classifier_performance.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def balanced_accuracy_expected(confusion):\n",
      "    \"\"\" Compute the expected value of the posterior balanced accuracy. Input is a numpy matrix\"\"\"\n",
      "    \n",
      "    alphas, betas = [], []\n",
      "    k = len(confusion) # number of classes\n",
      "    \n",
      "    for i in range(k):\n",
      "        # alpha is 1 plus the number of objects that are correctly classified\n",
      "        alphas.append(1 + confusion[i, i])\n",
      "        \n",
      "        # beta is 1 plus the number of objects that are incorrectly classified\n",
      "        betas.append(1 + confusion.sum(axis=1)[i] - confusion[i, i])\n",
      "    \n",
      "    parameters = zip(alphas, betas)\n",
      "    \n",
      "    # convolve the distributions and compute the expected value\n",
      "    res = 0.001\n",
      "    x = np.arange(0, k + res, res)\n",
      "    bal_accuracy = convolve_betas(parameters, res)\n",
      "    bal_accuracy_expected = (1/k) * np.dot(x, bal_accuracy * res)\n",
      "    \n",
      "    return bal_accuracy_expected\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "3. Recall"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can also compute the recall of each class. The recall of class $i$ is defined as:\n",
      "\\begin{aligned}\n",
      "    \\text{Recall}_i = \\dfrac{C_{ii}}{\\sum_j C_{ij}}\n",
      "\\end{aligned}\n",
      "\n",
      "Intuitively, the recall measures a classifier's ability to find all the positive samples (and hence minimising the number of false negatives)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load -s recall scripts/classifier_performance.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def recall(confusion, classes, classifiers):\n",
      "    \"\"\"\n",
      "    Input: confusion matrix\n",
      "           array of class names (row index)\n",
      "           array of classifier names (column index)\n",
      "    \"\"\"\n",
      "\n",
      "    # initialise dict to store results\n",
      "    results_dict = {c: {} for c in classes}\n",
      "    \n",
      "    # extract recall from confusion matrix\n",
      "    for clf, conf in confusions.items():\n",
      "        for i in range(len(classes)):\n",
      "            results_dict[classes[i]] = conf[i, i] / conf.sum(axis=1)[i]\n",
      "\n",
      "    # organise as DataFrame\n",
      "    results_df = DataFrame.from_dict(results_dict, orient=\"index\")\n",
      "    results_df = results.df.reindex(columns=classifiers, copy=False)\n",
      "    \n",
      "    return results_df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "4. Precision"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Another useful measure is the precision. The precision of class $i$ is defined as:\n",
      "\\begin{aligned}\n",
      "    \\text{Precision}_i = \\dfrac{C_{ii}}{\\sum_j C_{ji}}\n",
      "\\end{aligned}\n",
      "\n",
      "Intuitively, the precision measures a classifier's ability to minimise the number of false positives."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load -s precision scripts/classifier_performance.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def precision(confusion, classes, classifiers):\n",
      "    \"\"\"\n",
      "    Input: confusion matrix\n",
      "           array of class names (row index)\n",
      "           array of classifier names (column index)\n",
      "    \"\"\"\n",
      "\n",
      "    # initialise dict to store results\n",
      "    results_dict = {c: {} for c in classes}\n",
      "    \n",
      "    # extract recall from confusion matrix\n",
      "    for clf, conf in confusions.items():\n",
      "        for i in range(len(classes)):\n",
      "            results_dict[classes[i]] = conf[i, i] / conf.sum(axis=0)[i]\n",
      "\n",
      "    # organise as DataFrame\n",
      "    results_df = DataFrame.from_dict(results_dict, orient=\"index\")\n",
      "    results_df = results.df.reindex(columns=classifiers, copy=False)\n",
      "    \n",
      "    return results_df\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
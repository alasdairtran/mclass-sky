{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### COMP4670/8600 - Introduction to Statistical Machine Learning - Tutorial 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\trace}[1]{\\operatorname{tr}\\left\\{#1\\right\\}}$\n",
    "$\\newcommand{\\Norm}[1]{\\lVert#1\\rVert}$\n",
    "$\\newcommand{\\RR}{\\mathbb{R}}$\n",
    "$\\newcommand{\\inner}[2]{\\langle #1, #2 \\rangle}$\n",
    "$\\newcommand{\\DD}{\\mathscr{D}}$\n",
    "$\\newcommand{\\grad}[1]{\\operatorname{grad}#1}$\n",
    "$\\DeclareMathOperator*{\\argmin}{arg\\,min}$\n",
    "\n",
    "Setting up the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the SciPy implementation of the logistic sigmoid function, rather than (naively) implementing it ourselves, to avoid issues relating to numerical computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.optimize as opt\n",
    "from scipy.special import expit # The logistic sigmoid function \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data set\n",
    "\n",
    "We will predict the incidence of diabetes based on various measurements (see [description](https://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes)). Instead of directly using the raw data, we use a normalised version, where the label to be predicted (the incidence of diabetes) is in the first column. Download the data from [mldata.org](http://mldata.org/repository/data/download/csv/diabetes_scale/).\n",
    "\n",
    "Read in the data using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diabetes</th>\n",
       "      <th>num preg</th>\n",
       "      <th>plasma</th>\n",
       "      <th>bp</th>\n",
       "      <th>skin fold</th>\n",
       "      <th>insulin</th>\n",
       "      <th>bmi</th>\n",
       "      <th>pedigree</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.294118</td>\n",
       "      <td>0.487437</td>\n",
       "      <td>0.180328</td>\n",
       "      <td>-0.292929</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.001490</td>\n",
       "      <td>-0.531170</td>\n",
       "      <td>-0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.882353</td>\n",
       "      <td>-0.145729</td>\n",
       "      <td>0.081967</td>\n",
       "      <td>-0.414141</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.207153</td>\n",
       "      <td>-0.766866</td>\n",
       "      <td>-0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.058824</td>\n",
       "      <td>0.839196</td>\n",
       "      <td>0.049180</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.305514</td>\n",
       "      <td>-0.492741</td>\n",
       "      <td>-0.633333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.882353</td>\n",
       "      <td>-0.105528</td>\n",
       "      <td>0.081967</td>\n",
       "      <td>-0.535354</td>\n",
       "      <td>-0.777778</td>\n",
       "      <td>-0.162444</td>\n",
       "      <td>-0.923997</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.376884</td>\n",
       "      <td>-0.344262</td>\n",
       "      <td>-0.292929</td>\n",
       "      <td>-0.602837</td>\n",
       "      <td>0.284650</td>\n",
       "      <td>0.887276</td>\n",
       "      <td>-0.600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   diabetes  num preg    plasma        bp  skin fold   insulin       bmi  \\\n",
       "0         0 -0.294118  0.487437  0.180328  -0.292929 -1.000000  0.001490   \n",
       "1         1 -0.882353 -0.145729  0.081967  -0.414141 -1.000000 -0.207153   \n",
       "2         0 -0.058824  0.839196  0.049180  -1.000000 -1.000000 -0.305514   \n",
       "3         1 -0.882353 -0.105528  0.081967  -0.535354 -0.777778 -0.162444   \n",
       "4         0 -1.000000  0.376884 -0.344262  -0.292929 -0.602837  0.284650   \n",
       "\n",
       "   pedigree       age  \n",
       "0 -0.531170 -0.033333  \n",
       "1 -0.766866 -0.666667  \n",
       "2 -0.492741 -0.633333  \n",
       "3 -0.923997 -1.000000  \n",
       "4  0.887276 -0.600000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = ['diabetes', 'num preg', 'plasma', 'bp', 'skin fold', 'insulin', 'bmi', 'pedigree', 'age']\n",
    "data = pd.read_csv('diabetes_scale.csv', header=None, names=names)\n",
    "data['diabetes'].replace(-1, 0, inplace=True) # The target variable need be 1 or 0, not 1 or -1\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification via Logistic Regression\n",
    "\n",
    "Implement binary classification using logistic regression for a data set with two classes. Make sure you use appropriate [python style](https://www.python.org/dev/peps/pep-0008/) and [docstrings](https://www.python.org/dev/peps/pep-0257/).\n",
    "\n",
    "Use ```scipy.optimize.fmin_bfgs``` to optimise your cost function. ```fmin_bfgs``` requires the cost function to be optimised, and the gradient of this cost function. Implement these two functions as ```cost``` and ```grad``` by following the equations in the lectures.\n",
    "\n",
    "Implement the function ```train``` that takes a matrix of examples, and a vector of labels, and returns the maximum likelihood weight vector for logistic regresssion. Also implement a function ```test``` that takes this maximum likelihood weight vector and the a matrix of examples, and returns the predictions. See the section **Putting everything together** below for expected usage.\n",
    "\n",
    "We add an extra column of ones to represent the constant basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 10)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['ones'] = np.ones((data.shape[0], 1)) # Add a column of ones\n",
    "data.head()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Set-up\n",
    "We have 9 input variables $x_0, \\dots, x_8$ where $x_0$ is the dummy input variable fixed at 1. (The fixed dummy input variable could easily be $x_5$ or $x_8$, it's index is unimportant.) We set the basis functions to the simplest choice $\\phi_0(\\mathbf{x}) = x_0, \\dots, \\phi_8(\\mathbf{x}) = x_8$. Our model then has the form\n",
    "$$\n",
    "  y(\\mathbf{x}) = \\sigma(\\sum_{j=0}^{8} w_j x_j) = \\sigma(\\mathbf{w}^T \\mathbf{x}.)\n",
    "$$\n",
    "Here we have a dataset, $\\{(\\mathbf{x}_n, t_n)\\}_{n=1}^{N}$ where $t_n \\in \\{0, 1\\}$, with $N=768$ examples. We train our model by finding the parameter vector $\\mathbf{w}$ which minimizes the (data-dependent) cross-entropy error function\n",
    "$$\n",
    "  E_D(\\mathbf{w}) =  - \\sum_{n=1}^{N} \\{t_n \\ln \\sigma(\\mathbf{w}^T \\mathbf{x}_n) + (1 - t_n)\\ln(1 - \\sigma(\\mathbf{w}^T \\mathbf{x}_n))\\}.\n",
    "$$\n",
    "The gradient of this function is given by\n",
    "$$\n",
    "  \\nabla E(\\mathbf{w}) = \\sum_{i=1}^{N} (\\sigma(\\mathbf{w}^T \\mathbf{x}_n) - t_n)\\mathbf{x}_n.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cost(w, X, y, c=0):\n",
    "    \"\"\"\n",
    "    Returns the cross-entropy error function with (optional) sum-of-squares regularization term.\n",
    "    \n",
    "    w -- parameters\n",
    "    X -- dataset of features where each row corresponds to a single sample\n",
    "    y -- dataset of labels where each row corresponds to a single sample\n",
    "    c -- regularization coefficient (default = 0)\n",
    "    \"\"\"\n",
    "    outputs = expit(X.dot(w)) # Vector of outputs (or predictions)\n",
    "    return -( y.transpose().dot(np.log(outputs)) + (1-y).transpose().dot(np.log(1-outputs)) ) + c*0.5*w.dot(w)\n",
    "\n",
    "def grad(w, X, y, c=0):\n",
    "    \"\"\"\n",
    "    Returns the gradient of the cross-entropy error function with (optional) sum-of-squares regularization term.\n",
    "    \"\"\"\n",
    "    outputs = expit(X.dot(w))\n",
    "    return X.transpose().dot(outputs-y) + c*w\n",
    "    \n",
    "def train(X, y,c=0):\n",
    "    \"\"\"\n",
    "    Returns the vector of parameters which minimizes the error function via the BFGS algorithm.\n",
    "    \"\"\"\n",
    "    initial_values = np.zeros(X.shape[1]) # Error occurs if inital_values is set too high\n",
    "    return opt.fmin_bfgs(cost, initial_values, fprime=grad, args=(X,y,c))\n",
    "\n",
    "def predict(w, X):\n",
    "    \"\"\"\n",
    "    Returns a vector of predictions. \n",
    "    \"\"\"\n",
    "    return expit(X.dot(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance measure\n",
    "\n",
    "There are many ways to compute the [performance of a binary classifier](http://en.wikipedia.org/wiki/Evaluation_of_binary_classifiers). The key concept is the idea of a confusion matrix or contingency table:\n",
    "\n",
    "|              |    | Label |    |\n",
    "|:-------------|:--:|:-----:|:--:|\n",
    "|              |    |  +1   | -1 |\n",
    "|**Prediction**| +1 |    TP | FP |\n",
    "|              | -1 |    FN | TN |\n",
    "\n",
    "where\n",
    "* TP - true positive\n",
    "* FP - false positive\n",
    "* FN - false negative\n",
    "* TN - true negative\n",
    "\n",
    "Implement three functions, the first one which returns the confusion matrix for comparing two lists (one set of predictions, and one set of labels). Then implement two functions that take the confusion matrix as input and returns the **accuracy** and **balanced accuracy** respectively. The [balanced accuracy](http://en.wikipedia.org/wiki/Accuracy_and_precision) is the average accuracy of each class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def confusion_matrix(predictions, y): \n",
    "    \"\"\"\n",
    "    Returns the confusion matrix [[tp, fp], [fn, tn]].\n",
    "    \n",
    "    predictions -- dataset of predictions (or outputs) from a model\n",
    "    y -- dataset of labels where each row corresponds to a single sample\n",
    "    \"\"\"\n",
    "    tp, fp, fn, tn = 0, 0, 0, 0\n",
    "    predictions = predictions.round().values # Converts to numpy.ndarray\n",
    "    y = y.values\n",
    "    for prediction, label in zip(predictions, y):\n",
    "        if prediction == label: \n",
    "            if prediction == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        else: \n",
    "            if prediction == 1:\n",
    "                fp += 1\n",
    "            else: \n",
    "                fn += 1\n",
    "    return np.array([[tp, fp], [fn, tn]])\n",
    "\n",
    "def accuracy(cm):\n",
    "    \"\"\"\n",
    "    Returns the accuracy, (tp + tn)/(tp + fp + fn + tn).  \n",
    "    \"\"\"\n",
    "    return cm.trace()/cm.sum()\n",
    "\n",
    "def positive_pred_value(cm):\n",
    "    \"\"\"\n",
    "    Returns the postive predictive value, tp/p.\n",
    "    \"\"\"\n",
    "    return cm[0,0]/(cm[0,0] + cm[0,1])\n",
    "    \n",
    "def negative_pred_value(cm):\n",
    "    \"\"\"\n",
    "    Returns the negative predictive value, tn/n.\n",
    "    \"\"\"\n",
    "    return cm[1,1]/(cm[1,0] + cm[1,1])\n",
    "    \n",
    "def balanced_accuracy(cm): \n",
    "    \"\"\"\n",
    "    Returns the balanced accuracy, (tp/p + tn/n)/2.\n",
    "    \"\"\"\n",
    "    return (cm[0,0]/(cm[0,0] + cm[0,1]) +  cm[1,1]/(cm[1,0] + cm[1,1]))/2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting everything together\n",
    "\n",
    "Consider the following code, which trains on all the examples, and predicts on the training set. Discuss the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 361.722693\n",
      "         Iterations: 18\n",
      "         Function evaluations: 30\n",
      "         Gradient evaluations: 30\n",
      "[-1.04704922 -3.49878957  0.81102853 -0.03063881  0.50408756 -3.00946746\n",
      " -1.10680533 -0.44607073  0.19501805]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.78255208333333337, 0.76912964680456408]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = data['diabetes']\n",
    "X = data[['num preg', 'plasma', 'bp', 'skin fold', 'insulin', 'bmi', 'pedigree', 'age', 'ones']]\n",
    "theta_best = train(X, y)\n",
    "print(theta_best)\n",
    "pred = predict(theta_best, X)\n",
    "cmatrix = confusion_matrix(pred, y)\n",
    "[accuracy(cmatrix), balanced_accuracy(cmatrix)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To aid our discussion we give the positive predictive value (PPV) and negative predictive value (NPV) also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.79892280071813282, 0.73933649289099523]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[positive_pred_value(cmatrix), negative_pred_value(cmatrix)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "Overall, the accuracy of our model is reasonable, given our naive choice of basis functions, as is its balanced accuracy. The discrepancy between these values can be accounted for by the PPV being higher than the NPV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (optional) Effect of regularization parameter\n",
    "\n",
    "By splitting the data into two halves, train on one half and report performance on the second half. By repeating this experiment for different values of the regularization parameter $\\lambda$ we can get a feeling about the variability in the performance of the classifier due to regularization. Plot the values of accuracy and balanced accuracy for at least 3 different choices of $\\lambda$. Note that you may have to update your implementation of logistic regression to include the regularisation parameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 175.645824\n",
      "         Iterations: 18\n",
      "         Function evaluations: 28\n",
      "         Gradient evaluations: 28\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 175.791523\n",
      "         Iterations: 18\n",
      "         Function evaluations: 28\n",
      "         Gradient evaluations: 28\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 177.046702\n",
      "         Iterations: 17\n",
      "         Function evaluations: 28\n",
      "         Gradient evaluations: 28\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 178.946282\n",
      "         Iterations: 17\n",
      "         Function evaluations: 28\n",
      "         Gradient evaluations: 28\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 181.702441\n",
      "         Iterations: 18\n",
      "         Function evaluations: 28\n",
      "         Gradient evaluations: 28\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 186.174289\n",
      "         Iterations: 16\n",
      "         Function evaluations: 28\n",
      "         Gradient evaluations: 28\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 189.748564\n",
      "         Iterations: 19\n",
      "         Function evaluations: 29\n",
      "         Gradient evaluations: 29\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 191.302245\n",
      "         Iterations: 16\n",
      "         Function evaluations: 28\n",
      "         Gradient evaluations: 28\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 192.733938\n",
      "         Iterations: 16\n",
      "         Function evaluations: 28\n",
      "         Gradient evaluations: 28\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 204.480698\n",
      "         Iterations: 16\n",
      "         Function evaluations: 27\n",
      "         Gradient evaluations: 27\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 213.232914\n",
      "         Iterations: 15\n",
      "         Function evaluations: 26\n",
      "         Gradient evaluations: 26\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 214.838433\n",
      "         Iterations: 15\n",
      "         Function evaluations: 26\n",
      "         Gradient evaluations: 26\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 216.290481\n",
      "         Iterations: 15\n",
      "         Function evaluations: 25\n",
      "         Gradient evaluations: 25\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 225.175215\n",
      "         Iterations: 17\n",
      "         Function evaluations: 26\n",
      "         Gradient evaluations: 26\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 244.154472\n",
      "         Iterations: 18\n",
      "         Function evaluations: 32\n",
      "         Gradient evaluations: 32\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 247.824279\n",
      "         Iterations: 17\n",
      "         Function evaluations: 32\n",
      "         Gradient evaluations: 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>regularization coefficient</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>balanced accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.765625</td>\n",
       "      <td>0.741143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.765625</td>\n",
       "      <td>0.741143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.765625</td>\n",
       "      <td>0.741822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.770833</td>\n",
       "      <td>0.750104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.770833</td>\n",
       "      <td>0.750104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.770833</td>\n",
       "      <td>0.751127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.50</td>\n",
       "      <td>0.770833</td>\n",
       "      <td>0.751127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.75</td>\n",
       "      <td>0.763021</td>\n",
       "      <td>0.742502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.00</td>\n",
       "      <td>0.757812</td>\n",
       "      <td>0.736642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5.00</td>\n",
       "      <td>0.765625</td>\n",
       "      <td>0.752150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>9.00</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.747225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10.00</td>\n",
       "      <td>0.755208</td>\n",
       "      <td>0.758935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11.00</td>\n",
       "      <td>0.755208</td>\n",
       "      <td>0.762150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>20.00</td>\n",
       "      <td>0.726562</td>\n",
       "      <td>0.748771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>100.00</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.707895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>150.00</td>\n",
       "      <td>0.658854</td>\n",
       "      <td>0.330287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    regularization coefficient  accuracy  balanced accuracy\n",
       "0                         0.00  0.765625           0.741143\n",
       "1                         0.01  0.765625           0.741143\n",
       "2                         0.10  0.765625           0.741822\n",
       "3                         0.25  0.770833           0.750104\n",
       "4                         0.50  0.770833           0.750104\n",
       "5                         1.00  0.770833           0.751127\n",
       "6                         1.50  0.770833           0.751127\n",
       "7                         1.75  0.763021           0.742502\n",
       "8                         2.00  0.757812           0.736642\n",
       "9                         5.00  0.765625           0.752150\n",
       "10                        9.00  0.750000           0.747225\n",
       "11                       10.00  0.755208           0.758935\n",
       "12                       11.00  0.755208           0.762150\n",
       "13                       20.00  0.726562           0.748771\n",
       "14                      100.00  0.666667           0.707895\n",
       "15                      150.00  0.658854           0.330287"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_data(data):\n",
    "    \"\"\"\n",
    "    Randomly split data into two equal groups.\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    N = len(data)\n",
    "    idx = np.arange(N)\n",
    "    np.random.shuffle(idx)\n",
    "    train_idx = idx[:int(N/2)]\n",
    "    test_idx = idx[int(N/2):]\n",
    "\n",
    "    X_train = data.loc[train_idx].drop('diabetes', axis=1)\n",
    "    t_train = data.loc[train_idx]['diabetes']\n",
    "    X_test = data.loc[test_idx].drop('diabetes', axis=1)\n",
    "    t_test = data.loc[test_idx]['diabetes']\n",
    "    \n",
    "    return X_train, t_train, X_test, t_test\n",
    "\n",
    "def reg_coefficient_comparison(reg_coefficients, X_train, t_train, X_test, t_test):\n",
    "    \"\"\"\n",
    "    Returns the accuracy and balanced accuracy for the given regularization coefficient values.\n",
    "    \n",
    "    reg_coefficients -- list of regularization coefficient values\n",
    "    X_train -- the input dataset used for training\n",
    "    t_train -- the dataset of labels used for training\n",
    "    X_test -- the input dataset used to make predictions from the trained model\n",
    "    t_test -- dataset of labels for performance assessment \n",
    "    \"\"\"\n",
    "    summary = []\n",
    "    for c in reg_coefficients:\n",
    "        w_best = train(X_train, t_train, c)\n",
    "        predictions = predict(w_best, X_test)\n",
    "        cm = confusion_matrix(predictions, t_test)\n",
    "        summary.append([c, accuracy(cm), balanced_accuracy(cm)])\n",
    "    return pd.DataFrame(summary, columns=[\"regularization coefficient\", \"accuracy\", \"balanced accuracy\"])\n",
    "\n",
    "X_train, t_train, X_test, t_test = split_data(data)\n",
    "reg_coefficients = [0, 0.01, 0.1, 0.25, 0.5, 1, 1.5, 1.75, 2, 5, 9, 10, 11, 20, 100, 150]\n",
    "reg_coefficient_comparison(reg_coefficients, X_train, t_train, X_test, t_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "It appears to be the case that accuracy is maximized for a regularization coefficient of approximately 1, while balanced accuracy is maximized for a regularization coefficient of approximately 11."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "Here we discuss possible approaches to improve our predictions. We made the decision to set the basis functions to the simplest choice $\\phi_0(\\mathbf{x}) = x_0, \\dots, \\phi_8(\\mathbf{x}) = x_8$. It is possible that making use of nonlinear basis functions, for instance, polynomial basis functions, may improve our predictive ability. This then raises the question of how to choose appropriate basis functions given that for data higher than 2 or 3 dimensions it is difficult to make choices based off straight-forward visualization. From the [description](https://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes) of the dataset we know also that their was missing data. \n",
    "\n",
    "\"Until 02/28/2011 this web page indicated that there were no missing values in the dataset. As pointed out by a repository user, this cannot be true: there are zeros in places where they are biologically impossible, such as the blood pressure attribute. It seems very likely that zero values encode missing data. However, since the dataset donors made no such statement we encourage you to use your best judgement and state your assumptions.\"\n",
    "\n",
    "It is likely that if our dataset were more complete, our model would have stronger predictive abilities.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

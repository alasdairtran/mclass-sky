\documentclass[11pt]{report}

\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{cleveref}
\usepackage{color}
\usepackage{fontspec}
\usepackage{mathtools}
\usepackage[separate-uncertainty=true]{siunitx}

\allowdisplaybreaks

\setmainfont{Palatino Linotype}

\DeclareSIUnit{\parsec}{pc}

% Define common symbols
\newcommand\bw{\mathbf{w}}
\newcommand\bx{\mathbf{x}}
\newcommand\by{\mathbf{y}}
\newcommand\bphi{\bm{\phi}}
\newcommand\bbE{\mathbb{E}}
\newcommand\bbN{\mathbb{N}}
\newcommand\bbR{\mathbb{R}}
\newcommand\cD{\mathcal{D}}
\newcommand\cG{\mathcal{G}}
\newcommand\cN{\mathcal{N}}
\newcommand\cP{\mathcal{P}}
\newcommand\cS{\mathcal{S}}
\newcommand\cX{\mathcal{X}}
\newcommand\cov{\mathrm{cov}}
\newcommand\var{\mathrm{var}}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\newcommand\jakub[1]{{\color{red}JAKUB: #1}}

\title{Optimising Experiments for Photometric Redshift Prediction}
\author{Jakub Nabaglo, u5558578}
\date{October 2017}

\begin{document}
\maketitle

\tableofcontents

\chapter{Introduction}
A redshift is the change in the wavelength of a photon emitted by an object that is moving away from the observer. When applied to the visible spectrum, a redshift makes an object appear more red.

Redshifts are important in astronomy, as they are used for a variety of tasks. The most important of these tasks is finding distances to objects.

Currently, redshifts are found using specroscopy. A spectroscope is applied to a galaxy, and its spectral profile is measured. From these measurements, we can isolate the hydrogen lines, which are then compared against the baseline to obtain the redshift. This method is problematic as it requires a spectroscope to be pointed at each galaxy whose redshift we wish to measure. Since there are many galaxies in the universe, it is impossible to use this method for large extents of the sky.

Instead, we can utilise a technique called `photometric redshift' prediction. A photometric redshift is one computed from photometric measurements, as opposed to spectroscopic ones. Photometric measurements `bucket' the spectrum into a small number of distinct intervals, much like a photographic camera buckets the visible spectum into red, green, and blue. These buckets can then be used as input to a regressor, and the redshift can be predicted.

Photometric have their own disadvantages. Currently, they only cover a limited range of redshifts. Further, they tend to rely on limited datasets, whose spectra have been measured without much regard to the impact of the sample on the prediction.

It therefore is appropriate to apply optimal design techniques to the problem. Optimal design allows us to choose, from a pool of unlabelled galaxies (i.e., galaxies for which we know the photometric data, but not the exact redshift from spectroscopic data), the one that would bring the most value to the regression problem if it were labelled. In other words, towards which galaxy should we point the spectroscope next so as to give us the most new information?

This method of selection of new training samples has advantages of greater cost effectiveness. It will also permit us to increase the range of current photometric redshift models with the lowest effort.

Optimal design requires us to be able to assess the uncertainties in the prediction for each point in the input space. This provides a useful heuristic for choosing the next sample to label.


\chapter{Photometric redshifts}

\section{Redshift}
Redshift is a property of all signals we observe from astronomical objects. It is often not easy to measure, but finding it permits us to learn important facts about the object being observed.

The electromagnetic waves that we observe from distant objects are subject to the Doppler effect. This is the phenomenon by which waves produced by an object that is moving away from the observer appear to have a longer wavelength. Consider an ambulance driving away from you: the tone of the siren appears lower than if the ambulance was stationary. In a similar way, a galaxy moving away from us appears redder, giving rise to the term `redshift'. The converse effect, affecting objects moving towards the observer, is termed `blueshift'.

The redshift of an object is then directly correlated to its velocity perpendicular to the line of sight from the observer. For a redshift $z$, the line-of-sight velocity is\[
    v \approx cz \text{,}
\] where $c$ is the speed of light constant.

This permits us to approximate the distance from the observed object. Due to the expansion of space, distant objects are moving away from us at a velocity proportional to their distance. For a velocity $v$, that distance is \[
    D \approx \frac{v}{H_0} \text{,}
\] where $H_0$ is the Hubble parameter, equalling approximately $\SI{70}{\kilo\meter\per\second\per\mega\parsec}$. For faraway objects then, the redshift is hence proportional to distance.

Since the speed of light is finite, electromagnetic waves coming from distant objects represent the past. Hence, observing distant objects permits us to study the history of the universe. For example, the universe is denser at larger distances, as a remnant of the earlier stages of the big bang. Similarly, large distances contain more quasars and more brighter objects. The ability to compute distance from the redshift is important in studying the past of the universe as the distance is a proxy for the age of the object we observe.

The mass of an object is correlated to its luminosity\footnote{the total amount of light emitted}. The luminosity can be approximated from the brightness using the object's distance. Hence, computing the distance of an object from the redshift permits us to approximate its size.

However, for some objects, we can approximate the size more directly. For closer objects, we can measure their apparent radius as the angle they span on the sky. Using trigonometry, we can then compute the actual radius of such an object from its apparent radius and redshift-derived distance.

For closer galaxies, we are able to observe the movements of stars inside the galaxy. When a star orbits the centre of a galaxy, the internal motion can cancel or amplify the star's line-of-sight velocity. Measuring the stars' velocities from redshift hence permits us to to study galaxy dynamics.

\chapter{Regression on Photometric Redshifts}
Let $f$ be an unobserved function mapping $\cX$ to $\bbR$, where $\cX \subseteq \bbR^d$ for some $d \in \bbN$. Regression is the task of reconstructing such $f$ from finitely many samples. As a complication, these samples may be noisy.

More formally, we are given a set of $n$ samples\[
    \cD = \{(\bx_1, y_1), \dots (\bx_n, y_n)\} \text{.}
\] For each $i = 1, \dots, n$, \[
    y_i = f(\bx_i) + \epsilon \text{ for some error } \epsilon \sim \cP \text{,}
\] where $\cP$ is some probability distribution of the error.

We wish to find a function $\hat f : \bbR^d \to \bbR$ that maximises the likelihood \[
    p\left(\by \;\middle|\; \hat f, X\right) \text{,}
\] where we define\[
    \by \coloneqq \begin{bmatrix} y_1 \\ \vdots \\ y_n \end{bmatrix}
    \text{ and }
    X \coloneqq \begin{bmatrix} \bx_1^\intercal \\ \vdots \\ \bx_n^\intercal \end{bmatrix}
\]

The formalisation of this likelihood is dependent on our assumptions. For example, it will depend on whether we know $\cP$, we assume it, or it is given to us.

In our case we will always assume that \[
    \cP = \cN(0, \sigma^2)
\] for some variance $\sigma^2$.

\section{Linear Regression}

Linear regression is the simplest regression model.

Choose $m \in \bbN$. Choose $\phi_1, \dots, \phi_m$, each mapping $\cX$ to $\bbR$.

We make the assumption that $\hat f$ is a linear combination of $\phi_1, \dots, \phi_m$. Then \[
    \hat f(\bx) = \bphi(\bx)^\top \bw \text{,}
\] where we define \[
    \bphi(\bx) \coloneqq \begin{bmatrix} \phi_1(\bx) \\ \vdots \\ \phi_n(\bx) \end{bmatrix}\text{,}
\] and where $\bw \in \bbR^m$ is the weights vector we wish to find.

Define also\[
    \Phi \coloneqq \begin{bmatrix}
        \bphi(\bx_1)^\top \\
        \vdots \\
        \bphi(\bx_n)^\top
    \end{bmatrix} \text{.}
\]

We also make the assumption that $p(\by \mid \hat f, X)$ depends only on how well $\hat f$ approximates $f$ at $x_1, \dots, x_n$. More formally, defining \[
    \hat y_i \coloneqq f(\bx_i) \text{ for }i = 1, \dots, n\text{ and }
    \hat\by \coloneqq \begin{bmatrix} \hat y_1 \\ \vdots \\ \hat y_n \end{bmatrix} \text{,}
\] we have that
\begin{align}
    p(\by \mid \hat f, X) &= \prod_{i = 1}^{n} \cN\left(\hat y_i \;\middle|\; y_i, \sigma^2\right) \label{eq:linearlikelihood} \\
    &= \prod_{i = 1}^{n} \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{\left(\hat y_i - y_i\right)^2}{2\sigma^2}\right) \text. \nonumber
\end{align} We wish to find $\bw$ that maximises this likelihood. This is equivalent to minimising the negative log likelihood,\begin{align*}
    -\log p(\by \mid \hat f, X) &= - \log \prod_{i = 1}^{n} \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{\left(\hat y_i - y_i\right)^2}{2\sigma^2}\right) \\
    &= - \sum_{i = 1}^{n} \log\left(\frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{\left(\hat y_i - y_i\right)^2}{2\sigma^2}\right)\right) \\
    &= - \sum_{i = 1}^{n} \left(\log\frac{1}{\sqrt{2\pi\sigma^2}} + \log\exp\left(-\frac{\left(\hat y_i - y_i\right)^2}{2\sigma^2}\right)\right) \\
    &= - \sum_{i = 1}^{n} \left(-\frac{1}{2}\log\left(2\pi\sigma^2\right) -\frac{\left(\hat y_i - y_i\right)^2}{2\sigma^2}\right) \\
    &=  \frac{n}{2}\log\left(2\pi\sigma^2\right) + \frac{1}{2\sigma^2}\sum_{i = 1}^{n}\left(\hat y_i - y_i\right)^2 \text{.}
\end{align*} Since $\frac{n}{2}\log(2\pi\sigma^2)$ is a constant, minimising $-\log p(\by \mid \hat f, X)$ is equivalent to minimising $\frac{1}{2\sigma^2}\sum_{i = 1}^{n}(\hat y_i - y_i)^2$. Further, since $1/\sigma^2$ is constant, this is equivalent to minimising $\frac12\sum_{i = 1}^{n}(\hat y_i - y_i)^2$.

We call this our loss function and write\[
    E(\bw) = \frac12\sum_{i = 1}^{n}(\hat y_i(\bw) - y_i)^2 \text{.}
\] Note that we can compute it without knowing $\sigma^2$. For reasons that should be clear, this is called the \textit{sum of squares} error.

The optimal weights vector is then given by \[
    \argmax_{\bw} p(\by \mid \hat f(\bw), X) = \argmin_{\bw} E(\bw) \text{.}
\]

We can solve this analytically. We first note that \begin{align*}
    E(\bw) &= \frac12\sum_{i = 1}^{n}(\bphi(\bx_i)^\top \bw - y_i)^2 \\
    &= \frac12\sum_{i = 1}^{n}(\Phi \bw - \by)_i^2 \\
    &= \frac12(\Phi \bw - \by)^\top(\Phi \bw - \by) \text{.}
\end{align*} Setting the derivative to zero to find the minimum, \[
    \frac{dE(\bw)}{d\bw} = \Phi^\top (\Phi\bw - \by) = \mathbf{0} \text{.}
\] Solving, the minimum is \[
    \bw  = \left(\Phi^\top\Phi\right)^{-1}\Phi^\top\by
\]

\subsection{Regularisation}
Often we wish to limit the complexity of $\hat f$ to prevent overfitting. We can make the assumption that every element of the weights vector $\bw$ is likely to be close to zero: \[
    p(w_i) = \cN(w_i \mid 0, \mu)
\] for some variance $\mu > 0$.

We can then change the likelihood in \cref{eq:linearlikelihood} to account for this: \begin{align*}
    p(\by \mid \hat f, X) &= \prod_{i = 1}^{n} \cN\left(\hat y_i \;\middle|\; y_i, \sigma^2\right) \label{eq:linearlikelihood} \prod_{i = 1}^{m}\cN(w_i \mid 0, \mu) \\
    &= \prod_{i = 1}^{n} \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{\left(\hat y_i - y_i\right)^2}{2\sigma^2}\right) \prod_{i = 1}^{m}\frac{1}{\sqrt{2\pi\mu}} \exp\left(-\frac{w_i^2}{2\mu}\right) \text{.}
\end{align*}

Computing the negative log likelihood,\[
    -\log p(\by \mid \hat f, X) = \frac{n}{2}\log\left(2\pi\sigma^2\right) + \frac{1}{2\sigma^2}\sum_{i=1}^{n}\left(\hat y_i - y_i\right)^2 + \frac{m}{2}\log\left(2\pi\mu\right) + \frac{1}{2\mu}\sum_{i=1}^{m}w_i^2 \text{.}
\]

Eliminating the constant terms, we find that \begin{align*}
    \argmin_{\bw} - \log p\left(\by \;\middle|\; \hat f, X\right) &= \argmin_{\bw} \left(\frac{1}{2\sigma^2}\sum_{i=1}^{n}\left(\hat y_i - y_i\right)^2 + \frac{1}{2\mu}\sum_{i=1}^{m}w_i^2\right) \\
    &= \argmin_{\bw} \frac{1}{\sigma^2}\left(\frac12\sum_{i=1}^{n}\left(\hat y_i - y_i\right)^2 + \frac{\lambda}{2}\sum_{i=1}^{m}w_i^2\right) \text{,}
\end{align*} where we defined $\lambda \coloneqq \sigma^2 / \mu > 0$. Eliminating the positive multiplicative constant,\begin{align*}
    \argmin_{\bw} - \log p\left(\by \;\middle|\; \hat f, X\right) &= \argmin_{\bw} \left(\frac12\sum
    _{i=1}^{n}\left(\hat y_i - y_i\right)^2 + \frac{\lambda}{2}\sum_{i=1}^{m}w_i^2\right) \\
    &= \argmin_{\bw} \left(\frac12\left(\hat \by - \by\right)^\top\left(\hat \by - \by\right) + \frac{\lambda}{2}\bw^\top\bw\right) \\
    &= \argmin_{\bw} \left(\frac12\left(\Phi \bw - \by\right)^\top\left(\Phi \bw - \by\right) + \frac{\lambda}{2}\bw^\top\bw\right) \text.
\end{align*}

Our loss now depends on $\bw$ and $\lambda$ and is given by \[
    E(\bw, \lambda) = \frac12\left(\Phi \bw - \by\right)^\top\left(\Phi \bw - \by\right) + \frac{\lambda}{2}\bw^\top\bw \text{,}
\] where $\lambda$ is our \textit{regularisation constant}: a higher value implies a simpler model, reducing the likelihood of overfitting, but increasing the chance of underfitting.

We can find $\bw$ that minimises the loss. Taking the derivative and setting it to $\mathbf{0}$,\[
    \frac{dE(\bw, \lambda)}{d\bw} = \Phi^\top(\Phi\bw - \by) + \lambda \bw = \mathbf{0} \text{.}
\] Solving,\[
    \bw = \left(\Phi^\top\Phi + \lambda I\right)^{-1}\Phi^\top\by \text.
\]

\section{Kernels}

\subsection{Kernel Approximations}

\subsubsection{Empirical Kernel Map}

\subsection{Rational Quadratic Kernel}

\subsection{Radial Basis Function Kernel}

\subsection{Matern Kernel}


\subsection{Kernelised Linear Regression}




\section{Gaussian Processes}
Consider the task of reconstructing a function $f: \bbR^d \to y$ from finitely many samples. As a complication, each target sample contains is noisy, with the noise having a variance $\sigma^2$.

More formally, we are given a finite set of tuples\[
    \{ (\vec{x_1}, y_1), \dots, (\vec{x_n}, y_n) \}\text,
\] where \[
    y_i \sim \cN\left(f(\vec{x_i}), \sigma^2\right)\text.
\] We wish to be able to predict $f(\vec{x})$, given an arbitrary $\vec{x}$.

A Gaussian process is a distribution over functions. Formally, a Gaussian process is defined as a `collection of random variables, any finite number of which have a joint Gaussian distribution.'\footnote{C. E. Rasmussen \& C. K. I. Williams, Gaussian Processes for Machine Learning, the MIT Press, 2006, ISBN 026218253X.}

Defining \begin{align*}
    m(\vec{x}) &\coloneqq \bbE[f(\vec{x})] \text, \\
    k(\vec{x}, \vec{x}') &\coloneqq \bbE[(f(\vec{x}) - m(\vec{x}))(f(\vec{x'}) - m(\vec{x'}))],
\end{align*} we write \[
    f(\vec{x}) \sim \cG\cP(m(\vec{x}), k(\vec{x}, \vec{x'}))\text.
\]

We write \begin{align*}
    \vec y &\coloneqq (y_1, \dots, y_n)^\intercal \text, \\
    X &\coloneqq (\vec{x_1}, \dots, \vec{x_n})^\intercal \text, \\
    K(X, X) &\coloneqq \begin{bmatrix}
        k(\vec{x_1}, \vec{x_1}) & \cdots & k(\vec{x_1}, \vec{x_n}) \\
        \vdots & \ddots & \vdots \\
        k(\vec{x_n}, \vec{x_1}) & \cdots & k(\vec{x_n}, \vec{x_n})
    \end{bmatrix}\text.
\end{align*}

Then $\cov(\vec{y}) = K(X, X) + \sigma_n^2I$.

Let $\vec{f_*}$ be the test predictions and $X_*$ be the test inputs. Under the prior, we have that the joint distribution of the observed targets and of the function values at the test locations is \[
    \begin{bmatrix}
        \vec{y} \\ \vec{f_*}
    \end{bmatrix} \sim \cN\left(\vec0, \begin{bmatrix}
        K(X, X) + \sigma_n^2I & K(X, X_*) \\
        K(X_*, X) & K(X_*, X_*)
    \end{bmatrix}\right)\text.
\]

It can then be derived that \[
    \vec{f_*} \mid X, \vec{y}, X_* \sim \cN\left(\overline{\vec{f_*}}, \cov(\vec{f_*})\right)\text,
\] where \[
    \overline{\vec{f_*}} = K(X_*, X)[K(X, X) + \sigma_n^2I]^{-1}\vec{y}\text,
\] and \[
    \cov(\vec{f_*}) = K(X_*, X_*) - K(X_*, X)[K(X, X) + \sigma_n^2I]^{-1}K(X, X_*)\text.
\]

The value for $\vec{f_*}$ that has the highest probability is the mean of the Gaussian from which it is sampled. Hence, we can predict that \begin{equation}
    \label{eq:pred}
    \vec{f_*} \approx K(X_*, X)[K(X, X) + \sigma_n^2I]^{-1}\vec{y}\text,
\end{equation} with the variance for each element of the vector being\[
    \var(\vec{f_*}_i) = \left( K(X_*, X_*) - K(X_*, X)[K(X, X) + \sigma_n^2I]^{-1}K(X, X_*) \right)_{ii}\text.
\]

One can observe from \cref{eq:pred} that the prediction is a linear combination of the observations $\vec{y}$. This is in contrast to linear regression, where the prediction is a linear combination of the inputs.

\subsection{Equivalence to Kernelised Linear Regression}

\section{Approximating Gaussian Processes}

\subsection{My Favourite Approximation}

\jakub{wtf is it called}

\subsection{Linear Regression and KDE}


\end{document}


%%
%% Template chap2.tex
%%

\chapter{Photometric Classification}
\label{cha:ml}

The two most common types of celestial objects are stars and galaxies. There are also some other
interesting objects such as quasars and white dwarfs. Quasars are thought to be supermassive black
holes surrounded by an accretion disc. They are very luminous and, unlike galaxies, appear as
single-source objects like stars. White dwarfs are low to intermediate mass stars that are in their
final evolutionary stage. They are very dense and have a faint luminosity.

One way to classify objects into these various groups is to manually inspect their spectra. There
have even been attempts to make the process more automatic. For example, \citeN{hala14} achieved a
95\% accuracy rate by training a convolutional neural network on one-dimensional spectra to classify
objects into stars, quasars, and galaxies. Even so, it is currently not possible to obtain a
spectrum of every object, especially faint ones. This means only a small number of objects (e.g.
less than 0.35\% in the SDSS dataset) can be classified this way. For the rest, we only have
photometric measurements.

Fortunately, the field of machine learning came about to solve this kind of problem. In the most
basic set-up, we have a set of objects, each with a vector of photometric measurements $\bm{x} \in
\X$. A subset of them forms the labelled set. Each object in this set has been spectroscopically
classified into some class $y \in \Y$. We call $\X$ the feature space and $\Y$ the label space.
During the training phase, we feed labelled examples to a classifier, which will attempt to form a
hypothesis $h: \X \rightarrow \Y$. These labelled examples form the training set and they are of
course not perfect, so the goal in machine learning is to capture as much of the underlying trend in
the data as possible, while avoiding fitting the random noise. Once trained, the classifier can then
be used to predict the classes of unlabelled objects.

\section{Classifiers}

\label{sec:machine} Three families of classifiers are used in our experiments. They are random
forests, logistic regression, and support vector machines. Below we give a quick overview of how
each of them works. We do not implement these classifiers ourselves. Rather we use scikit-learn
\cite{pedregosa11}, the most well-known machine learning package in Python.

\subsection{Random Forest}

To understand the motivation behind random forests, we first need to know how to construct
individual decision trees. Building each of these trees is like playing a game of Twenty Questions.
We start with the whole training set and at each step, we slice the feature space along the axis of
one feature. After many steps, we end up with a set of hyper-dimensional cuboids which form a
partition of the feature space. The algorithm stops when each of these cuboids contains data from
only one class. There are many criteria that we can use to decide on which feature and where we
slice along the axis. In this thesis, we use the Gini impurity which intuitively measures the
potential misclassification rate. In particular, let $k$ be the number of classes and $q_D(i)$ be
the frequency of objects belonging to class $i$ in set $D$. Then the Gini impurity of $D$ is the
probability that a randomly selected object from $D$ is misclassified if it were labelled according
to its frequency in $D$:
	\begin{IEEEeqnarray*}{lCl}
		\iota_G(D) &=& \sum_{i=1}^{k} q_D(i) (1 - q_D(i)) \\
		           &=& \sum_{i=1}^{k} q_D(i)  - \sum_{i=1}^{k} q_D(i)^2 \\
		           &=& 1 - \sum_{i=1}^{k} q_D(i)^2
	\end{IEEEeqnarray*}
When we partition $D$ into subsets $\{D_1, D_2, ..., D_d\}$, the Gini impurity of $D$ is now the sum
of the individual Gini impurities, weighted by the size of the subsets:
	\begin{IEEEeqnarray*}{lCl}
		\iota_G(D) &=& \sum_{i=1}^{d} \frac{|D_i|}{|D|} \iota_G(D_i)
	\end{IEEEeqnarray*}
Observe that if a subset contains objects from only one class, then its Gini impurity will be zero.
This gives us the following splitting criterion: at each step, slice the feature space along the
axis that will result in the greatest drop in the Gini impurity.

One problem with decision trees is that they tend to overfit the data and thus do not generalise
well. To solve this, \citeN{breiman01} proposes that we build many decision trees, thus creating a
random forest. The random forest makes its prediction by simply counting up the predictions of all
the individual trees and then taking the most popular choice. By averaging the predictions, we avoid
the problem of overfitting. Furthermore, for each tree, we only give it a small bootstrap sample and
at each split, we only consider a small number of features. This bootstrapping and random subspace
selection have been shown empirically to improve the accuracy rate \cite{breiman96, ho98, louppe12}.
Another nice feature of random forests is that they are extremely fast and hence scale well with
large datasets. Although they do not provide class probability estimates, we can use proportions of
the votes as a proxy for the likelihood that an object is in each class. However, as we shall see in
Appendix \ref{sec:forest_prob}, these probabilities are not very stable.

\subsection{Logistic Regression}

If we want to have better probability estimates, then an alternative approach is to use logistic
regression. Developed by \citeN{cox58}, the algorithm tries to directly model the probability of
being in a class. Let $\bm{x}$ be the feature vector and $\bm{\beta}$ be the vector of coefficients.
The linear predictor $\eta$ is defined as
	\begin{IEEEeqnarray*}{lCl}
		\eta(\bm{x}) &=& \bm{\beta}^T \bm{x}
	\end{IEEEeqnarray*}	
Since probabilities must lie between 0 and 1, we want our predictor to have the same range. This can
be achieved by wrapping $\eta$ around the logistic function:
	\begin{IEEEeqnarray*}{lCl}
		\sigma(\eta(\bm{x})) &=& \frac{1}{1 + e^{-\eta(\bm{x})}}
	\end{IEEEeqnarray*}
We can now interpret $\sigma(\eta(\bm{x}))$ as the probability that an object with feature vector
$\bm{x}$ belongs to the positive class. The goal of the algorithm is then to use the training data
to estimate the coefficient vector $\bm{\beta}$.

There are a few ways to extend this model to the multi-class setting. One option is the multinomial
logistic regression, where we would need to jointly solve a set of $(k-1)$ binary regressions if we
have $k$ classes. In practice, when running the multinomial option in scikit-learn, the probability
estimates are as unreliable as those of random forests. The cause of this is unknown, but it is more
likely due to flaws in the scikit-learn implementation than in the actual theory.\footnote{See
	Figure ~ in Appendix ~ for a plot comparing the learning curves of multinomial with one-vs-rest.} A
more empirically stable alternative is to use the one-vs-rest strategy, where we run $k$ independent
binary regressions. In particular, for class $i$, we pretend that the dataset contains only objects
from class $i$ and class `not $i$'. We then train the binary logistic model on this simplified
dataset and we do this $k$ times, one for each class. At the end, we end up with $k$ probabilities.
These can be interpreted like usual after normalisation.


\subsection{Support Vector Machines}

Support vector machines (SVM), first introduced by \citeN{boser92}, are another popular family of
algorithms. They have been used in astronomy, for example by \citeN{elting08}, to find non-linear
decision boundaries in the colour space of the SDSS dataset. The idea here is to find a decision
boundary that can maximise the distance between the boundary and the closest data points. This can
be done by solving a Lagrangian function. SVMs, in their original formulation, are inherently binary
classifiers. However we can still use the one-vs-rest strategy to extend it to the multi-class
setting. There has even been an attempt to derive an inherently multi-class SVM \cite{crammer02}.
\improvement{explan gamma and c}

\subsection{Learning Complex Hypotheses}

Both SVMs and logistic regression are linear classifiers. When dealing with real-world data like
those in astronomy, we should not expect to be able to separate classes with a hyper-dimensional
plane. If we want them to learn more complex hypotheses, one option is do an explicit polynomial
transformation of the original photometric measurements. When we give the classifier the transformed
features, it would still find a linear boundary in the transformed space. However, the boundary
would mostly be non-linear in the original space. A second option is to use the kernel trick which
does an implicit map into a high-dimensional feature space. For example, a popular kernel that is
often used with SVMs is the radial basis function (RBF), which actually maps the inputs into an
infinite-dimensional space.

In random forests, we do not have to worry about any transformation. Although in each round, we
slice the data along only one axis, there is no limit on how many slices we can take and how small
the resulting cuboids can be. This allows us to learn arbitrarily complex hypotheses.

\section{Overview of Active Learning}

We now turn our attention to the construction of the training set. Getting spectroscopic labels is
expensive. Until now, astronomers do not have a quantitative approach to inform them about which
objects to obtain spectra for. Often, they simply take a random sample of the sky. To see why this
might be a problem, imagine that there is a group of objects with very similar photometric
measurements. We can obtain spectra from all of them and conclude, for example, that they are all
stars. However, a smarter way is to get only one spectrum from this group for labelling and let the
classifier generalise to other similar objects. Keeping the size of the training set as small as
possible while not sacrificing the classifier accuracy is the goal of active learning.

There are three main types of active learning: membership query synthesis, stream-based selective
sampling, and pool-based active learning. In membership query synthesis, we are allowed to request
labels for any unlabelled instance in the feature space \cite{angluin88}. Equivalently, we may
request the astronomer to find an object with a certain combination of colours and magnitudes. This
is not very realistic since such objects might not even exist. In stream-based selective sampling,
we sample objects from the source one at a time, and as objects are streaming in, we must decide to
either label or discard each of them \cite{cohn94}. The assumption here is that it is free to obtain
unlabelled examples, which again is not applicable to astronomy. Thus we will not discuss membership
query synthesis and stream-based selective sampling further in this thesis.

Instead, we will focus on pool-based active learning \cite{lewis94}, the most relevant type of
active learning for astronomy. In this setting, we keep track of two sets. The labelled set
$\Labelled \subset \X \times \Y$ contains all examples that have been labelled by an expert so far.
All the remaining unlabelled examples form the unlabelled set $\Unlabelled \subset \X$. The question
now is how to select the next training example from $\Unlabelled$. In practical terms, where should
we next point the telescope to, in order to obtain a spectrum? To answer this question, we need a
rule $r(\bm{x}; h)$ that can assign a score to each object, based solely on their photometric
features and the current hypothesis. This score should reflect the amount of new information we
would gain if we were to label the object. Once we have computed $r(\bm{x}; h)$ for all candidates,
we can then pick the example with the highest score and obtain its spectrum. The object's features
and its label are then added to the training set and the classifier is retrained to obtain a new
$h$.

In general, it is very difficult to compute $r(\bm{x}; h)$. In fact, deriving an algorithm to find
this function is still an open problem. For now, the best that we can do is to come up with
heuristics. In practice, the unlabelled pool can be arbitrarily large. For example, there are 800
million unlabelled objects in the SDSS. Thus if we only have a limited computing power, at each
round, we might only be able to select a small sample of size $t$ where the score of each can be
calculated. A formal description of the active learning routine is given as follows. Note that for
some active learning rules, we might need to substitute $\argmax$ with $\argmin$ in line 4.

\algblock[Name]{Start}{End}

\algblockdefx[Forall]{Foreach}{Endforeach}%
			[1]{\textbf{for each} #1 \textbf{do}}%
			{\textbf{end for}}

\begin{algorithm}[h]
	\caption{The general pool-based active learning algorithm}
	\label{alg:active}
	\begin{algorithmic}[1]
		\Procedure {ActiveLearner}{$\Unlabelled$, $\Labelled$, $h$, $r$, $n$, $t$}
			\While {$|\Labelled| < n$}
				\State $E$ $\leftarrow$ random sample of size $t$ from $\Unlabelled$
				\State $\bm{x}_* \leftarrow \argmax_{\bm{x} \in E} r(\bm{x}; h)$
				\State $y_* \leftarrow$ ask the expert to label $\bm{x}_*$
				\State $\Labelled \leftarrow \Labelled  \cup (\bm{x}_*, y_*)$
				\State $\Unlabelled \leftarrow \Unlabelled \setminus \bm{x}_*$
				\State $h_\Labelled(\bm{x}) \leftarrow$ retrain the classifier
			\EndWhile
			\EndProcedure
	\end{algorithmic}
\end{algorithm}


\section{Active Learning Heuristics}
\label{sec:heuristics}

There are many ways to rank unlabelled objects. The four prominent families of heuristics are
uncertainty sampling, query by bagging, loss function minimisation, and classifier certainty
\cite{schein07}. All of these heuristics require the class probabilities estimated by the current
hypothesis. We now discuss each of them in turn, starting with the least computationally expensive
one.

\subsection{Uncertainty Sampling}

\citeN{lewis94} introduce the idea of uncertainty sampling, where we select the example whose class
membership the classifier is least certain about. These tend to be points that are near the decision
boundary of the classifier. To quantify the uncertainty, \citeN{schein07} propose two techniques,
entropy maximisation and margin minimisation.

The Shannon entropy measures the amount of information contained in some object $\bm{x}$ and is
defined as
	\begin{IEEEeqnarray*}{lCl}
		r_S(\bm{x}; h) &=& -\sum_{y \in \Y} p(y | \bm{x}; h)
							\log \big[ p(y | \bm{x}; h) \big]
	\end{IEEEeqnarray*}
Intuitively, the closer class probabilities of an object are to random guessing, the higher its
entropy will be. This gives us the heuristic of picking the candidate with the highest Shannon
entropy.

In fact, if we care about how close the class probabilities are to random guessing, there is an even
simpler measure. Let $y_1$ and $y_2$ be the two most likely classes for some object
$\bm{x}$. We define the margin as the difference between these two values:
	\begin{IEEEeqnarray*}{lCl}
		r_M(\bm{x}; h) &=& \abs{ p(y_1 | \bm{x}; h) - p(y_2 | \bm{x}; h) }
	\end{IEEEeqnarray*}
Since the class probabilities must add up to 1, the smaller the margin is,
the more uncertain we are. Thus another heuristic is
to pick the candidate with the smallest margin.


\subsection{Query by Bagging}

Instead of focussing on the uncertainty of individual predictions, we could look at the disagreement
among a group of classifiers. Let us define the version space as the set of all possible hypotheses
that are consistent with the current labelled training set. In query by bagging (QBB), before we can
assign a score to each candidate, we need to train a committee of classifiers, $\B = \{h_1, h_2,
..., h_B\}$, each with a hypothesis that is as different from the others as possible but that is
still in the version space \cite{melville04}. Minimising the disagreement is equivalent to
constraining the size of the version space, which will hopefully allow to search for the optimal
hypothesis more easily . In order to have the variation in the hypothesis, each committee member is
only given a subset of the labelled examples. Since there might be enough training data (for
example, in our experiments, we have 11 members but only a maximum of 300 labelled points), we need
to select samples with replacement (hence the term bagging).

Once we have constructed the committee, we pick the candidate whose class membership the committee
disagrees the most about about.  To quantify the disagreement, \citeN{melville04} extend the margin
approach from uncertainty sampling. This involves averaging out the class probabilities estimated by
the committee members and then calculating the margin, giving us the heuristic
	\begin{IEEEeqnarray*}{lCl}
		r_{QM}(\bm{x}; \B) &=& \abs{ p(y_1 | \bm{x}; \B) - p(y_2 | \bm{x}; \B) }
	\end{IEEEeqnarray*}
where $y_1$ and $y_2$ are the two most likely classes and
	\begin{IEEEeqnarray*}{lCl}
		p(y | \bm{x}; \B) &=& \dfrac{1}{B} \sum_{b=1}^{B} p(y | \bm{x}, h_b)
	\end{IEEEeqnarray*}
Since the bagging technique has been shown to improve the stability of the predictions
\cite{breiman96}, we should expect this method to be no worse the the simple margin approach. The
cost, however, is that it now takes $B$ times longer to calculate the scores, where $B$ is the size
of the committee.

In addition to the margin, \citeN{mccallum98} offer an alternative disagreement measure which
involves picking the candidate with the largest expected Kullback--Leibler (KL) divergence
from the average:
	\begin{IEEEeqnarray*}{lCl}
		r_{QK}(\bm{x}; \B) = \dfrac{1}{B} \sum_{b=1}^B D_{\mathrm{KL}}(p_b\|p_\B)
	\end{IEEEeqnarray*}
where 
	\begin{IEEEeqnarray*}{lCl}
		D_{\mathrm{KL}}(p_b\|p_\B) = \sum_{y \in \Y} p(y | \bm{x}; h_b) \,
		                             \ln\frac{p(y | \bm{x}; h_b)}{p(y | \bm{x}; \B)}
	\end{IEEEeqnarray*}
The KL divergence measures the amount of information lost when $p_\B$ is used to approximate $p_b$.
Intuitively, the larger the KL divergence is, the more disagreement there is between $p_\B$ and $p_b$. In
the active learning context, $p_\B$ is the average prediction probability of the committee, while $p_b$
is the prediction of a particular committee member. This gives us the heuristic of picking the
candidate with the largest expected KL divergence from the average:





\subsection{Variance Minimisation} We can in fact try to minimise a loss function explicitly. For
example, the expected squared loss can be decomposed into three terms:
	\begin{IEEEeqnarray*}{lCl}
		\E{\text{Squared Loss}} &=& \text{Bias}^2 + \text{Variance} + \text{Noise}
	\end{IEEEeqnarray*}
The noise is intrinsic in the data and represents the expected loss under the optimal hypothesis.
Thus there is nothing we can do about the noise. The squared bias can be minimised \cite{cohn97},
but we do not explore it here. Instead, let us focus on minimising the variance of the unlabelled
pool since the term will vanish as the training set size approaches infinity. The expected variance
of the model after an example $\bm{x}$ is labelled and added to the training set is
	\begin{IEEEeqnarray*}{lCl}
		r_V(\bm{x}; h)
		&=& \E{\text{V}(\Labelled \cup (\bm{x}, y); h)} \\
		&=& \sum_{y \in \Y} p(y | \bm{x}; h) \text{V}(\Labelled \cup (\bm{x}, y); h)
	\end{IEEEeqnarray*}
where the expectation is over the class probability distribution of $\bm{x}$ under the current
hypothesis and $\Labelled$ is the current training set. Note that the variance depends on the
hypotheses and hence on the training set. This makes calculating it an expensive computation, since
to assign a score to each candidate, we first need to give it each of the possible labels, add it to
the training set to get a updated hypothesis, and calculate the new variance.

In addition, estimating $\text{V}(\Labelled; h)$ requires a bit of work. In
multinomial logistic regression, the hypothesis maps features directly to class probabilities.
This allows \citeN{schein07} to take the first two terms of the Taylor series expansion
of probability
	\begin{IEEEeqnarray*}{lCl}
		p(y | \bm{x}, \bm{\hat{w}}, h)
		&\approx& p(y | \bm{x}, \bm{w}, h) + \mathbf{g}_{\bm{x}}(y)(\bm{\hat{w}} - \bm{w})
	\end{IEEEeqnarray*}
where $\bm{w}$ and $\bm{\hat{w}}$ are the expected and the current estimates of the model parameters,
and $\mathbf{g}_{\bm{x}}(y)$ is called the gradient vector.
If we let $F$ be the Fisher information matrix and
	\begin{IEEEeqnarray*}{lCl}
		A &=& \sum_{\bm{x} \in \Unlabelled}
		      \sum_{y \in \Y} \mathbf{g}_{\bm{x}}(y) ~ \mathbf{g}_{\bm{x}}(y)^T
	\end{IEEEeqnarray*}
and it can be shown that
	\begin{IEEEeqnarray*}{lCl}
		\text{V}(\Labelled; h) &=& tr(AF^{-1})
	\end{IEEEeqnarray*}
where the trace function $tr(X)$ is the sum of the elements along the main diagonal of a square
matrix $X$. See \citeN{schein07} for the detailed derivation. Note that this derivation is specific
to the multinomial logistic regression. If we use the one-vs-rest strategy with binary logistic
regression or another entirely different classifier like SVMs in our experiments, the same
approximation might not hold and we should not expect to get good results. We leave the variance
estimation of other learning algorithms for future work.



\subsection{Classifier Certainty}
Finally, instead of minimising the variance of the unlabelled pool, \citeN{mackay91} proposes
minimising the entropy of the classifier's predictions on $\Unlabelled$, which is calculated as
	\begin{IEEEeqnarray*}{lCl}
		CC(\Labelled; h) &=& - \sum_{\bm{x} \in \Unlabelled} \sum_{y \in \Y}
							 p(y | \bm{x}, h) \log \big[ p(y | \bm{x}, h)  \big]
	\end{IEEEeqnarray*}
Although this sounds
similar to one of the uncertainty sampling heuristics, here instead of picking the 
most uncertain candidate, we pick the candidate that is expected to increase the classifier's
prediction certainty by the the most amount:
	\begin{IEEEeqnarray*}{lCl}
		r_{CC}(\bm{x}; h)
		&=& \E{CC(\Labelled \cup (\bm{x}, y))} \\
		&=& \sum_{y\in \Y} p(y|\bm{x}; h)  CC(\Labelled \cup (\bm{x}, y))
	\end{IEEEeqnarray*}
Like the variance estimation, to get the score of just one candidate, we need to retrain the
classifier $k$ times, where $k$ is the number of classes. Thus in practice, both the variance
estimation and the classifier certainty heuristics might be too computationally expensive to run.

\subsection{Summary of Heuristics}
Let us call the six heuristics we have just described entropy, margin, QBB margin, QBB KL,
pool variance, and pool entropy, respectively. For convenience, we collect together
all the heuristics that we will in the experiments in Table \ref{tab:heuristics}.
\begin{table}[h]
	\caption {List of Heuristics} \label{tab:heuristics}
	\centering
	\begin{tabular}{lllc}
		\toprule
		{Name} & Objective & Notation &  Heuristic  \\
		\midrule
		Entropy & $\argmax$ & $r_S(\bm{x}; h)$
			& $-\sum_{y \in \Y} p(y | \bm{x}; h) \log \big[ p(y | \bm{x}; h) \big]$
			\\[2ex]
		Margin & $\argmin$ & $r_M(\bm{x}; h)$
			& $\abs{ p(y_1 | \bm{x}; h) - p(y_2 | \bm{x}; h) }$
			\\[2ex]
		QBB Margin & $\argmin$ & $r_{QM}(\bm{x}; h)$
			& $\abs{ p(y_1 | \bm{x}; \B) - p(y_2 | \bm{x}; \B) }$
			\\[2ex]
		QBB KL & $\argmax$ & $r_{QK}(\bm{x}; h)$
			& $\dfrac{1}{B} \sum_{b=1}^B D_{\mathrm{KL}}(p_b\|p_\B))$
			\\[2ex]
		Pool Variance & $\argmin$ & $r_V(\bm{x}; h)$
			& $\sum_{y \in \Y} p(y | \bm{x}; h) \text{V}(\Labelled \cup (\bm{x}, y); h)$
			\\[2ex]
		Pool Entropy & $\argmin$ & $r_{CC}(\bm{x}; h)$
			& $\sum_{y\in \Y} p(y|\bm{x}; h)  CC(\Labelled \cup (\bm{x}, y))$
			\\
		\bottomrule
	\end{tabular}
\end{table}


\section{Multi-arm Bandit}
\label{sec:bandit}

Out of the six heuristics, how do we know which  heuristic is the best, anyway? There have been some
attempts in the literature to do a theoretical analysis of them. However proofs are quite scarce,
and when there is one available, they normally only work under simplifying assumptions. For example,
\shortciteN{freund97} showed that query by committee heuristics (where we sample without
replacement) guarantee an exponential decrease in the prediction error with the training size, but
only in the noiseless case. Thus whether many of these are guaranteed to beat random sampling is
still an open question. We do not worry too much about theoretical analysis in this thesis. Instead
we focus on an empirical analysis in the astronomical domain.

To help us automatically choose the optimal heuristic, 
we now turn our attention to the multi-armed bandit problem in probability theory. The colourful
name originates from the situation where a gambler stands in front a slot machine with $n$ levers.
When pulled, each lever gives out a random reward according to some unknown distribution.
The goal of the game is to come up with a strategy that can maximise the gambler's
lifetime rewards with a minimum number of pulls.

The key novel contribution of this thesis is the application of this theory to the problem of
heuristic selection. Suppose we have a set of $n$ heuristics $ \R = \{r_1, ..., r_n \}$. Each heuristic
has a different ability to pick the candidate that can give the biggest gain in information
when added to the training set. An appropriate reward is then the incremental increase
in the accuracy rate. Observe that the heuristic rewards are independent of each other,
since the theories with which we use to derive the heuristics are mostly unrelated.

Let $\bm{w}$ be the reward vector where entry $\bm{w}_i$ is the reward of heuristic $r_i$.
Observe that even with the optimal heuristic, there could be error during the labelling
process that causes the accuracy rate to decrease. Conversely, a bad heuristic might be
able to pick an informative candidate due to pure luck. Thus there is always a certain level
of randomness in the reward received. These errors are probably normally distributed, so
	\begin{IEEEeqnarray*}{lCl}
		(\bm{w} \mid \bm{\nu}, \tau^2) \sim N(\bm{\nu}, \tau^2 \bm{I})
	\end{IEEEeqnarray*}
where $\bm{I}$ is the identity matrix.
To make the problem tractable, assume that we know the constant variance $\tau^2$. Assume that
the mean vector $\bm{\nu}$ follows a normal distribution
	\begin{IEEEeqnarray*}{lCl}
		\bm{\nu} \sim N(\bm{\mu}, \sigma^2 \bm{I})
	\end{IEEEeqnarray*}
Since we do not yet have any information about the performance of each heuristic,
the hyperparameters $\bm{\mu}$ and $\sigma^2$ are unknown.

One problem in multi-arm bandits is the trade-off between exploration and exploitation.
Suppose we have managed to estimate $\bm{\mu}$ and $\sigma^2$, then by always selecting
the heuristic with the highest possible $\bm{\mu}$, or the greedy heuristic,
we would be exploiting our current
knowledge. If however we select one of the other non-greedy heuristics, we would then be exploring
with the intention of improving our current estimates of $\bm{\mu}$ and $\sigma^2$. There are
many instances in which we find our previously held beliefs to be completely wrong. Thus
by always exploiting, we might miss out on the optimal heuristic. On the other hand,
if we explore too much, it might take a long time to reach the desired accuracy rate and
the strategy ends up being no different from random sampling. \info{relevant to ASKAP and to us anyway}

\section{Thompson Sampling}
\label{sec:thompson}

There are two main methods in the literature that address this exploration vs exploitation 
problem. The algorithm with a strong theoretical guarantee is Upper Confidence Bound
\cite{auer02}. We will, however, focus on a simpler and much older algorithm called Thompson sampling.
First introduced by \citeN{thompson33}, the algorithm solves the trade-off from
a Bayesian perspective. It has been shown to achieve results
that are comparable and sometimes even better than Upper Confidence Bound \cite{chapelle11}.

Under the heuristic selection setting, Thompson sampling works as follows.
We start with a prior knowledge of $\bm{\mu}$ and $\sigma^2$. In each
round, we draw a random sample from the distribution $N(\bm{\mu}, \sigma^2 \bm{I})$
and select heuristic $r_*$ that has the highest mean reward value in the sample. We then 
observe the actual reward $w_{*}$ received and use it to update the prior accordingly.
Under the normal likelihood, the normal distribution is conjugate to itself, and it can be
shown that the posterior distribution of the mean reward $\bm{\nu}_{*}$
remains normal, but now with parameters
	\begin{IEEEeqnarray*}{lCl}
		(\bm{\nu}_{*} \mid w_{*}) \sim N \left(
		\frac{\bm{\mu}_* \bm{\tau}_* + w_{r^*} \bm{\sigma}_*}{\bm{\tau}_* + \bm{\sigma}_*},
		\frac{\bm{\sigma}_* \bm{\tau}_*}{\bm{\tau}_* + \bm{\sigma}_*}
		\right)
	\end{IEEEeqnarray*}
Below is the formal specification of the algorithm.

\begin{algorithm}[h]
	\caption{Thompson sapmling}
	\label{alg:thompson}
	\begin{algorithmic}[1]
		\Procedure {ThompsonSampling}{$\R$, $\bm{\mu}$, $\bm{\sigma}$, $\bm{\tau}$, n}
		\Foreach {$t \in \{1, 2, ..., n\}$}
		\Foreach {$r \in \R$}
		\State $\bm{\nu}_r' \leftarrow$ draw a sample from $N(\bm{\mu_r}, \bm{\sigma_r})$
		\Endforeach
		\State $r_* \leftarrow \argmax_{r \in \R}\bm{\nu}_r'$
		\State Observe reward $w_{*}$
		\State Update $\bm{\mu}_{*}$
		\State Update $\bm{\sigma}_{*}$
		\Endforeach
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

If we combine Algorithm \ref{alg:active} and \ref{alg:thompson}, we end up with
an algorithm that can automatically select the optimal active learning heuristic
with Thompson sampling.

\begin{algorithm}[h]
	\caption{The multi-arm bandit active learning algorithm}
	\label{alg:bandit}
	\begin{algorithmic}[1]
		\Procedure {ActiveBandit}{$\Unlabelled$, $\Labelled$, $h$, $n$, $t$,
			                      $\R$, $\bm{\mu}$, $\bm{\sigma}$, $\bm{\tau}$}
		\While {$|\Labelled| < n$}
		\Foreach {$r \in \R$}
			\State $\bm{\nu}_r' \leftarrow$ draw a sample from $N(\bm{\mu_r}, \bm{\sigma_r})$
		\Endforeach
		\State $r_* \leftarrow \argmax_{r \in \R} \bm{\nu}_r'$
		\State $E$ $\leftarrow$ random sample of size $t$ from $\Unlabelled$
		\State $\bm{x}_* \leftarrow \argmax_{\bm{x} \in E} r_*(\bm{x})$
		\State $y_* \leftarrow$ ask the expert to label $\bm{x}_*$
		\State $\Labelled \leftarrow \Labelled  \cup (\bm{x}_*, y_*)$
		\State $\Unlabelled \leftarrow \Unlabelled \setminus \bm{x}_*$
		\State $h_\Labelled(\bm{x}) \leftarrow$ retrain the classifier
		\State $\delta$ $\leftarrow$ change in the balanced accuracy rate 
		\State Update $\bm{\mu}_{*}$
		\State Update $\bm{\sigma}_{*}$
		\EndWhile
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

As we shall see in Chapter \ref{cha:expt2}, the reward function dynamically evolves
as the training size increases. Intuitively, the accuracy rate can never go beyond 100\%, so
we would expect the incremental change in the accuracy rate to become smaller over time.
Attempts to address this problem have been made in the literature. For example \citeN{gupta11}
introduce the Dynamic Thompson Sampling method that manages to adapt to the evolving
parameters faster than Algorithm \ref{alg:thompson}. However, we shall leave the investigation
of such methods to future work.


% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 
\section{Performance Measures}
\label{sec:measures}

We end this chapter with a description of two performance measures that are used in the experiments.

\begin{figure}[tbp]
	\centering
	\renewcommand\arraystretch{1.5}
	\setlength\tabcolsep{0pt}
	\begin{tabular}{c >{\bfseries}r @{\hspace{0.7em}}c @{\hspace{0.4em}}c @{\hspace{0.4em}}c}
		\multirow{13}{*}{\rotatebox{90}{\parbox{1.1cm}{\bfseries\raggedleft Actual}}} & 
		& \multicolumn{3}{c}{\bfseries Predicted} \\
		& & \bfseries Galaxy & \bfseries Star & \bfseries Quasar \\
		& Galaxy & \MyBox{97,621}{95.7\%} & \MyBox{492}{0.5\%} & \MyBox{1,887}{1.8\%} \\[2.4em]
		& Star & \MyBox{1,625}{1.6\%} & \MyBox{89,489}{95.4\%}  & \MyBox{8,886}{8.5\%} \\[2.4em]
		& Quasar & \MyBox{2,790}{2.7\%} & \MyBox{3,868}{4.1\%}  & \MyBox{93,342}{89.7\%}
	\end{tabular}
	\caption[Confusion matrix of random forest on SDSS]{
		The confusion and the normalised confusion matrix of the random forest
		on the SDSS test set. For example, out of all the objects predicted as quasars, 8.5\%
		of them are actually stars. Out of all the objects predicted as stasr, 4.1\% of them
		are actually quasars.}
	\label{fig:recall}
\end{figure}

\subsection{Recall}
Let $C$ be the confusion matrix of a classifier on a test set,
where entry $C_{ij}$ is the number of objects in class $i$
but have been predicted to be in class $j$.
Recall measures the classifier's ability to find all the positive examples and avoid
having false negatives:
\begin{IEEEeqnarray*}{lCl}
	\text{Recall}_i &=& \frac{C_{ii}}{\sum_j C_{ij}}
\end{IEEEeqnarray*}
We use recall to plot the the performance of a classifier on the celestial sphere. This
measure is chosen mainly due to its simple implementation.

\subsection{Posterior Balanced Accuracy Rate} Certain astronomical objects are either rarer or more
difficult to detect than others. In the SDSS labelled set, there are 4.5 times as many galaxies as
quasars. The problem of class imbalance is even more severe in the VST-ATLAS set, with 11 times more
stars than white dwarfs. An easy fix is to undersample the dominant class when creating training and
test sets. This, of course, means that the size of these sets are limited by the size of the
minority class.

When we do not want to alter the underlying class distributions or when larger training and test
sets are desired, we need a performance measure that can correct for the class imbalance.
\shortciteN{brodersen10} showed that the posterior balanced accuracy distribution can overcome the
bias in the binary case. We now extend this idea to the multi-class setting.

Suppose we have $k$ classes. For each class $i$ between $1$ and $k$, there are $N_i$ objects in the
universe. Given a classifier, we can assign a predicted label to every object and compare our
prediction to the true label. Let $C_i$ be the number of objects in class $i$ that are correctly
predicted. Then we define the accuracy rate $A_i$ of class $i$ as
	\begin{IEEEeqnarray*}{lCl}
		A_i &=& \frac{C_i}{N_i}
	\end{IEEEeqnarray*}
Initially we have no information about $C_i$ and $N_i$, so we can assume that each $A_i$ 
follows a uniform prior from 0 to 1. This is the same as a Beta distribution
with parameters $\alpha = \beta = 1$:
	\begin{IEEEeqnarray}{lCl}
		A_i &\sim& \Beta(1,1) \label{eqn:prior}
	\end{IEEEeqnarray}
After we have trained the classifier, suppose we have a test set containing $n_i$
objects in class $i$. Running the classifier on this test set is the same as conducting
$k$ binomial experiments, where, in the $i$th experiment, the sample size is
$n_i$ and the probability of success is simply the accuracy rate $A_i$. Let $c_i$ be
the number of correctly labelled objects belonging to class $i$ in the test set. Then,
conditional on the accuracy rate, $c_i$ follows a binomial distribution:
	\begin{IEEEeqnarray}{lCl}
		(c_i \mid A_i) &\sim& \Bin(n_i, A_i) \label{eqn:likelihood}
	\end{IEEEeqnarray}
In the Bayesian setting, \eqref{eqn:prior} is the prior and \eqref{eqn:likelihood}
is the likelihood. To get the posterior PDF, we simply multiply the prior with the likelihood:
	\begin{IEEEeqnarray*}{lCl}
		f_{A_i \mid \bm{c}}(a)
		&\propto& f_{A_i}(a) \times f_{c_i \mid A_i}(c_i) \\
		&\propto& a^{1-1}(1-a)^{1-1} \times a^{c_i} (1 - a)^{n_i - c_i} \\
		&=& a^{1 + c_i - 1}(1-a)^{1 + n_i - c_i - 1}
	\end{IEEEeqnarray*}
Thus, with respect to the binomial likelihood function,
the Beta distribution is conjugate to itself. The posterior accuracy rate $A_i$
also follows a Beta distribution, now with parameters
	\begin{IEEEeqnarray*}{lCl}
		(A_i \mid c_i) &\sim& \Beta(1 + c_i, 1 + n_i - c_i)
	\end{IEEEeqnarray*}
Recall that our goal is to have a balanced accuracy rate, $A$, that puts an equal
weight in each class. This can be achieved by taking the average of all the class accuracy rates:
	\begin{IEEEeqnarray*}{lCl}
		A &=& \frac{1}{k} \sum_{i=1}^k A_i \\
		&=& \frac{1}{k} A_T
	\end{IEEEeqnarray*}
Here we have defined $A_T$ to be the sum of the individual accuracy rates.
We call  $(A \mid \bm{c})$ the posterior balanced accuracy rate, where
$\bm{c} =(c_1,...,c_k)$.
Most of the time, we simply want to calculate its expected value:
	\begin{IEEEeqnarray*}{lCl}
		\E{A \given \bm{c}} &=& \frac{1}{k} \, \E{A_T \given \bm{c}} \\
		&=& \frac{1}{k} \int a \cdot f_{A_T \mid \bm{c}}(a) \, da
	\end{IEEEeqnarray*}
Note that there is no closed form solution for the PDF $f_{A_T \mid \bm{c}}(a)$.
However assuming that $A_T$ is a sum of $k$ independent Beta random variables,
$f_{A_T \mid \bm{c}}(a)$ can be approximated by numerically convolving $k$ Beta distributions.
The independence assumption is reasonable here, since there should be little to no correlation
between the individual class accuracy rates. Knowing that a classifier is really good
at recognising stars does not tell us much about how well that classifier can recognise
galaxies.

Having the knowledge of $f_{A \mid \bm{c}}(a)$ will allow us to make violin plots,
construct confidence intervals and do hypothesis tests. To get an expression for this,
let us first rewrite the CDF as
	\begin{IEEEeqnarray*}{lCl}
		F_{A\mid \bm{c}}(a) &=& \Prob{A \leq a \mid \bm{c}} \\
		&=& \Prob[\Big]{\frac{1}{k} A_T \leq a \given \bm{c}} \\
		&=& \Prob{A_T \leq ka \given \bm{c}} \\
		&=& F_{A_T \mid \bm{c}}(ka) \IEEEyesnumber \label{eqn:CDF}
	\end{IEEEeqnarray*}
Differentiating \eqref{eqn:CDF} with respect to $a$, we obtain the PDF of $(A \mid \bm{c})$:
	\begin{IEEEeqnarray*}{lCl}
		f_{A \mid \bm{c}}(a) &=& \frac{\partial}{\partial a} F_{A \mid \bm{c}}(ka) \\
		&=& \frac{\partial}{\partial a} (ka) \cdot \frac{\partial}{\partial ka} F_{A_T \mid \bm{c}}(ka) \\
		&=& k \cdot f_{A_T \mid \bm{c}}(ka)
	\end{IEEEeqnarray*}
The posterior balanced accuracy rate is used throughout the experiments to report
the overall performance of a classifier. 

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 

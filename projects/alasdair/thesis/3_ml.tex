
%%
%% Template chap2.tex
%%

\chapter{Photometric Classification}
\label{cha:ml}

The two most common types of celestial objects are stars and galaxies. There are also some
interesting objects such as quasars and white dwarfs. Quasars are thought to be supermassive black holes surrounded by an accretion disks.
White dwarfs are stars in their final stage. One way to classify an object into 
these various groups is to manually inspect its spectrum.
There has even been attempts to make the process
more automatic. For example, \citeN{hala14} achieved a 95\% accuracy rate by training
a convolutional neural network on one-dimensional spectra to classify objects
into stars, quasars, and galaxies. Even so, it is currently not possible to obtain
a spectrum of every object, especially faint ones. This means only a small number of objects
(less than 0.05\% in the SDSS dataset) can be classified this way. For the rest,
we only have photometric measurements.

Fortunately, the field of machine learning came about to solve this kind of problem.
In the most basic set-up, we have a set of object, each with some photometric measurements.
These measurements form the feature space $\X$.
Some of the objects have been spectroscopically classified and these are used as training
examples. During the training phase, we feed these labelled examples into a classifier,
which will attempt to form a hypothesis. The labelled examples are of course not perfect, and so
the goal in machine learning is to capture as much of the underlying trend in the data as possible,
while avoiding fitting the random noise. Once trained, the classifier can then
be used to predict the classes of the unlabelled objects.

\section{Classifiers}
\label{sec:machine}
Three family of classifiers are used in the experiments. They are random forests,
logistic regression, and support vector machines. Below we give a quick overview of how
each of them works. We will not implement these algorithms ourselves. Rather we will use
scikit-learn \cite{pedregosa11}, the most well-known machine learning package
in Python. 

\subsection{Random Forest}
To understand how random forests work, we first need to construct individual trees.
Building a decision tree is like playing the a game of Twenty Questions. We start with the
whole training set and at each step, we slice the data along the axis of one feature. We
keep doing this until all the subsets contain data from only one class. There are many
criteria that we can use to decide on which feature and where we slice along the axis.
In this thesis, we use the Gini impurity which intuitively measures the potential misclassification
rate. In particular, let $k$ be the number of classes and $q_D(i)$ be the frequency of objects
belonging to class $i$ in set $D$. Then the Gini impurity of $D$ is the 
probability that a randomly selected object from $D$ is misclassified if it were labelled
according to its frequency in $D$:
	\begin{IEEEeqnarray*}{lCl}
		\iota_G(D) &=& \sum_{i=1}^{k} q_D(i) (1 - q_D(i)) \\
		           &=& \sum_{i=1}^{k} q_D(i)  - \sum_{i=1}^{k} q_D(i)^2 \\
		           &=& 1 - \sum_{i=1}^{k} q_D(i)^2
	\end{IEEEeqnarray*}
When we split set $D$ into two subsets $D_1$ and $D_2$, the Gini impurity of $D$ is now
the sum of the individual Gini impurities, weighted by the size of the subsets:
	\begin{IEEEeqnarray*}{lCl}
		\iota_G(D) &=& \frac{|D_1|}{|D|} \iota_G(D_1) + \frac{|D_2|}{|D|} \iota_G(D_2)
	\end{IEEEeqnarray*}
Observe that if a subset contains objects from only one class, then its Gini impurity would be
zero. This gives us the following splitting criterion: at each step, slice the data along the
axis that will result in the greatest drop in the Gini impurity.

One problem with decision trees is that they tend to overfit the data and thus do not generalise
well. To solve this, we can build many decision trees, thus creating a random forest.
The random forest makes its prediction by simply counting up all the
predictions of the individual trees and then taking the most popular choice. By averaging
the predictions, we avoid the problem of overfitting. Furthermore, for each tree,
we only give it a small bootstrap sample and at each split, we only consider a small number
of features. This bootstrapping and random subspace selection have been shown empirically
to improve the accuracy rate \cite{breiman96, ho98, louppe12}.
Another nice
feature of random forests is that they are extremely fast and hence scale well with large
datasets. Although they do not provide true probability estimates, we can use the proportion
of the votes as a proxy of the likelihood that an object is in some particular class. 

\subsection{Logistic Regression}
If we want to have true probability estimates, then logistic regression might be a better option.
Developed by \citeN{cox58}, the algorithm tries to directly model the probability of 
being in a class. Let $\bm{x}$ be a vector of features and $\bm{\beta}$ be the vector of 
coefficients. The linear predictor is defined as
	\begin{IEEEeqnarray*}{lCl}
		\eta(\bm{x}) &=& \bm{\beta}^T \bm{x}
	\end{IEEEeqnarray*}	
Since probabilities must lie between 0 and 1, we want our predictor to have the same range.
This can be achieved by wrapping $\eta$ around the logistic function:
	\begin{IEEEeqnarray*}{lCl}
		\sigma(\eta(\bm{x})) &=& \frac{1}{1 + e^{-\eta(\bm{x})}}
	\end{IEEEeqnarray*}
We can now interpret $\sigma(\eta(\bm{x}))$ as the probability that an object with feature
vector $\bm{x}$ belonging to the positive class. The goal of the algorithm is to use the
training data to estimate the coefficient vector $\bm{\beta}$.

There are a few ways to extend this model to the multi-class setting. One option is
the multinomial logistic regression, where we would need to jointly solve a set of $(k-1)$ 
binary regressions if we have $k$ classes. In practice, when running the multinomial setting
in scikit-learn, we do not get reliable probability estimates. This is probably due
to rounding errors created by the implementation.\footnote{See Figure ~ in
	Appendix ~ for a plot comparing the learning curves of multinomial with one-vs-rest.}
A more empirically stable alternative is to use the one-vs-rest strategy, where we run $k$ 
independent binary regressions. In particular, for class $i$, we pretend that the dataset
contains only objects from class $i$ and class `not $i$'. We then train the normal logistic model
on this simplified dataset and we do this $k$ times, one for each class. At the end, we end up
with $k$ probabilities. These can be interpreted like usual after normalisation.


\subsection{Support Vector Machines}

Support vector machines, first introduced by \citeN{boser92}, are another popular
family of algorithms. They have been used in astronomy, for example by \citeN{elting08},
to find non-linear decision boundaries in the colour space of the SDSS dataset. The idea
here is to find the decision boundary that can maximise the distance between the boundary and the
closest data points. Support vector machines are inherently binary classifiers, but
we still use the one-vs-rest strategy to extend it to the multi-class setting.

\section{Overview of Active Learning}

We now turn our attention to the construction of the training set.
Getting spectroscopic labels is expensive. Until now, when astronomers pick which objects to obtain
spectra from, not much attention is paid to whether the labelling information can improve
the performance of the classifiers. To make it more concrete, imagine there is a group of objects
with very similar photometric measurements. We can obtain spectra from all of them and conclude
that they are all stars. However, a smarter way is to get only one spectrum for labelling and 
let the classifier generalise to other similar objects. Keeping the size of the training set
as small as possible while not sacrificing the classifier accuracy is the goal of active
learning.

We will focus on pool-based active learning, the most relevant type of active learning
for astronomy. In this setting, we keep track of two sets. The labelled
set $\Labelled \subset \X \times \Y$ contains all the examples that have been labelled
by an expert so far. All the remaining unlabelled examples form $\Unlabelled \subset \X$.
The question now is how to select the next training example from $\Unlabelled$. In practical terms, 
where should we next point the telescope to, in order to obtain a spectrum? To answer this question,
we need a rule $r(\bm{x})$ that can assign a score to each object, based solely on their
photometric
features. The score should reflect the amount of new information we would gain if we were to label the object.
We will discuss various heuristics in the next section. Once we have computed $r(\bm{x})$ for
all candidates,
we can now pick the example with the highest score and obtain its spectrum. The object
and its label is then added to the training set and the classifier is retrained.
Algorithm \ref{alg:active} shows the active learning routine in more details.

\algblock[Name]{Start}{End}

\algblockdefx[Forall]{Foreach}{Endforeach}%
			[1]{\textbf{for each} #1 \textbf{do}}%
			{\textbf{end for}}

\begin{algorithm}[tbp]
	\caption{The general pool-based active learning algorithm}
	\label{alg:active}
	\begin{algorithmic}[1]
		\Procedure {ActiveLearner}{$\Unlabelled$, $\Labelled$, $h$, $r$ $n$, $t$}
			\While {$|\Labelled| < n$}
				\State $E$ $\leftarrow$ random sample of size $t$ from $\Unlabelled$
				\State $\bm{x}^* \leftarrow \argmax_{\bm{x} \in E} r(\bm{x})$
				\State $y^* \leftarrow$ ask the expert to label $\bm{x}^*$
				\State $\Labelled \leftarrow \Labelled  \cup (\bm{x}^*, y^*)$
				\State $\Unlabelled \leftarrow \Unlabelled \setminus \bm{x}^*$
				\State $h_\Labelled(\bm{x}) \leftarrow$ retrain the classifier
			\EndWhile
			\EndProcedure
	\end{algorithmic}
\end{algorithm}


\section{Active Learning Heuristics}
\label{sec:heuristics}

There are many ways to rank unlabelled objects. The four prominent families
of heuristics are uncertainty sampling, query by bagging, loss function minimisation, and
and classifier certainty \cite{schein07}. All of these heuristics require the
class probabilities estimated by the current classifier. 
We now discuss each of them in turn, starting with the least computationally expensive
one. We will now discuss each family in turn.
(To do: Also margin-based method 
provides an exponential improvement \cite{balcan12})

\subsection{Uncertainty Sampling}

\citeN{lewis94} introduces the idea of uncertainty sampling, where we select the example
whose class membership the classifier is least certain about.
These tend to be points that are near the decision boundary of the classifier. To quantify
the uncertainty, \citeN{schein07} propose two techniques, entropy maximisation and
margin minimisation.

The Shannon entropy measures the amount of information contained in some object $\bm{x}$
and is defined as
	\begin{IEEEeqnarray*}{lCl}
		r_S(\bm{x}) &=& -\sum_i^k \Prob{y(\bm{x}) = i} \log \big[\Prob{y(\bm{x}) = i} \big]
	\end{IEEEeqnarray*}
where $k$ is the number of classes. Intuitively, the closer class probabilities of an object
are to random guessing, the higher its entropy will be. This gives us the heuristic
of picking the candidate with the highest Shannon entropy.

In fact, if we care about how close the class probabilities are to random guessing, 
there is an even simpler measure. 
Let $c^{(1)}$ and $c^{(2)}$ be the two most likely classes for some object $\bm{x}$.
We define the margin as the difference between these two values:
	\begin{IEEEeqnarray*}{lCl}
		r_M(\bm{x}) &=& \Big|  \Prob{y(\bm{x}) = c^{(1)}} -  \Prob{y(\bm{x}) = c^{(2)}} \Big|
	\end{IEEEeqnarray*}
Since the class probabilities must add up to 1, the smaller the margin is,
the more uncertain we are. Thus another heuristic is
to pick the candidate with the smallest margin.


\subsection{Query by Bagging}

Instead of focussing the uncertainty of individual predictions, we could look at
the disagreement among a group of classifiers. In query by bagging (QBB), before we
can assign a score to each candidate, we need to train a committee of classifiers, each
with a hypothesis that is as different from the others as possible but that is still consistent
with the training data \cite{melville04}. In order to have this variation,
for each committee member, we only give it a subset of the labelled examples. Since there
might be enough training data (in our experiments, we have 11 members but only a maximum 
of 300 labelled points), we need to select a sample with replacement (hence the term bagging).

Once we've constructed the committee, we pick the candidate whose class membership the committee 
disagrees the most about about. To quantify the disagreement, \citeN{melville04} extends the
margin approach above. This involves averaging out the class probabilities estimated
by the committee members and then calculating the margin. Since the bagging technique
has been shown to improve the stability of the predictions \cite{breiman96}, we should
expect this method to be no worse the the simple margin approach. The cost, however, is that
it now takes $B$ times longer to calculate the scores, where $B$ is the size of the 
committee.

In addition ot the margin, \citeN{mccallum98} offer an alternative disagreement measure.
We first need to define the Kullback–Leibler (KL) divergence. Suppose we have two discrete
probability distributions $P$ and $Q$, then the KL divregence of $P$ from $Q$ is
	\begin{IEEEeqnarray*}{lCl}
		D_{\mathrm{KL}}(P\|Q) = \sum_i P(i) \, \ln\frac{P(i)}{Q(i)}
	\end{IEEEeqnarray*}
This KL divergence measures the amount of information lost when $Q$ is used to approximate $P$. In the active learning context, $Q$ is the average prediction probability of the committee, while $P$ is the prediction of a particular committee member.
This allows us to pick the candidate with the largest value of the average Kullback-Leibler divergence from the average:
	\begin{IEEEeqnarray*}{lCl}
		\dfrac{1}{B} \sum_{b=1}^B \text{KL}(f_b \mid\mid f_\text{avg})
	\end{IEEEeqnarray*}




\subsection{Variance Minimisation}
We can in fact try to minimise a loss function explicitly. For example, for the square loss,
we can decompose it into three terms, the bias, the variance, and te noise. The noise is an
inherent part of the data and there is not much we can do about it.
The bias can be minimised \cite{cohn97}, however
we will not explore it here. We will instead focus on minimising the variance, which will
vanish as the training set size grows to infinity.
Using Taylor series approximation, \citeN{schein07} estimates the variance in multinomial logistic regression as $tr(AF^{-1})$, where $F$ is the Fisher information matrix and $A$ is defined as:
	\begin{IEEEeqnarray*}{lCl}
		A &=& \sum_n \sum_c \mathbf{g}_n(c) ~ \mathbf{g}_n(c)^T
	\end{IEEEeqnarray*}
where $\mathbf{g}_n(c)$ is the gradient vector. Our implementation involves a vectorised computation of $A$ and $F$.


\subsection{Classifier Certainty}
Finally, we can pick the example that will minimise the expected entropy of the algorithm's prediction. The entropy can be computed as follows:
	\begin{IEEEeqnarray*}{lCl}
		CC &=& - \sum_{p \in \text{Pool}} \sum_c p(c \mid \mathbf{x}_p) \log p(c \mid \mathbf{x}_p)
	\end{IEEEeqnarray*}
	



\section{Multi-arm Bandit}

Define arms, rewards, exploration-exploitation trade-off.

\section{Thompson Sampling}
We now turn our attention to the problem heuristic selection.
 One of the oldest idea which we will
study here is Thompson sampling, first introduced by \citeN{thompson33}.
\citeN{chapelle11} showed that it is a highly effective algorithm and can perform better
than Upper Confidence Bound, a very popular algorithm that have been studied extensively.
Algorithm \ref{alg:thompson} shows how Thompson Sampling works in the active learning
setting.

\begin{algorithm}[tbp]
	\caption{Thompson Sapmling}
	\label{alg:thompson}
	\begin{algorithmic}[1]
		\Procedure {ThompsonSampling}{$\R$, $\bm{\mu}$, $\bm{\sigma}$, $\bm{\tau}$}
		\Foreach {$t \in \{1, 2, ..., n\}$}
			\Foreach {$r \in \R$}
				\State $W(r) \leftarrow$ draw a sample from $N(\bm{\mu_r}, \bm{\sigma_r})$
			\Endforeach
			\State $r^* \leftarrow \argmax_{r \in \R} W(r)$
			\State Observe reward $w_t$
			\State Update $\bm{\mu}_{r^*}$
			\State Update $\bm{\sigma}_{r^*}$
		\Endforeach
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

We assume the rewards are normally distributed with prior
	\begin{IEEEeqnarray*}{lCl}
		\bm{w} \sim N(\bm{\mu}, \sigma \bm{I})
	\end{IEEEeqnarray*}
Let us also assume that the likelihood follows a Normal distribution. This means
that the posterior is also normal:
	\begin{IEEEeqnarray*}{lCl}
		(\bm{w}_i \mid \bm{\tau}) \sim N \left(
			\frac{\bm{\mu}_i \bm{\tau}_i + \delta \bm{\sigma}_i}{\bm{\tau}_i + \bm{\sigma}_i},
			\frac{\bm{\sigma}_i \bm{\tau}_i}{\bm{\tau}_i + \bm{\sigma}_i}
			\right)
	\end{IEEEeqnarray*}

\begin{algorithm}[tbp]
	\caption{The multi-arm bandit active learning algorithm}
	\label{alg:bandit}
	\begin{algorithmic}[1]
		\Procedure {ActiveBandit}{$\Unlabelled$, $\Labelled$, $h$, $n$, $t$,
			                      $\R$, $\bm{\mu}$, $\bm{\sigma}$, $\bm{\tau}$}
		\While {$|\Labelled| < n$}
		\State $E$ $\leftarrow$ random sample of size $t$ from $\Unlabelled$
		\Foreach {$r \in \R$}
			\State $W(r) \leftarrow$ draw a sample from $N(\bm{\mu_r}, \bm{\sigma_r})$
		\Endforeach
		\State $r^* \leftarrow \argmax_{r \in \R} W(r)$
		\State $\bm{x}^* \leftarrow \argmax_{\bm{x} \in E} r^*(\bm{x})$
		\State $y^* \leftarrow$ ask the expert to label $\bm{x}^*$
		\State $\Labelled \leftarrow \Labelled  \cup (\bm{x}^*, y^*)$
		\State $\Unlabelled \leftarrow \Unlabelled \setminus \bm{x}^*$
		\State $h_\Labelled(\bm{x}) \leftarrow$ retrain the classifier
		\State $\delta$ $\leftarrow$ change in the balanced accuracy rate 
		\State Update $\bm{\mu}_{r^*}$
		\State Update $\bm{\sigma}_{r^*}$
		\EndWhile
		\EndProcedure
	\end{algorithmic}
\end{algorithm}


To do: apply Thompson sampling to drifting rewards \cite{gupta11}


% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 
\section{Performance Measures}
\label{sec:measures}

\subsection{Posterior Balanced Accuracy Rate}
Certain astronomical objects are either rarer or more difficult to detect than others.
In the SDSS labelled set, there are 4.5 times as many galaxies as quasars. The problem
of class imbalance is even more severe in the VST-ATLAS set, with 11 times more stars than
white dwarfs. An easy fix is to undersample the dominant class when creating training and
test sets. This, of course, means that the size of these sets are limited by the size
of the minority class.

When we do not want to alter the underlying class distributions or when larger training and test
sets are desired, we need a performance measure that can correct for the class imbalance.
\shortciteN{brodersen10} showed that the posterior balanced accuracy distribution can overcome
the bias in the binary case. We now extend this idea to the multi-class setting.

Suppose we have $k$ classes. For each class $i$ between $1$ and $k$, there are $N_i$ objects
in the universe. Given a classifier, we can assign a predicted label to every object and
compare our prediction to the true label. Let $C_i$ be the number of objects in class $i$
that are correctly predicted. Then we define the accuracy rate $A_i$ of class $i$ as
	\begin{IEEEeqnarray*}{lCl}
		A_i &=& \frac{C_i}{N_i}
	\end{IEEEeqnarray*}
Initially we have no information about $C_i$ and $N_i$, so we can assume that each $A_i$ 
follows a uniform prior from 0 to 1. This is the same as a Beta distribution
with parameters $\alpha = \beta = 1$:
	\begin{IEEEeqnarray}{lCl}
		A_i &\sim& \Beta(1,1) \label{eqn:prior}
	\end{IEEEeqnarray}
After we have trained the classifier, suppose we have a test set containing $n_i$
objects in class $i$. Running the classifier on this test set is the same as conducting
$k$ binomial experiments, where, in the $i$th experiment, the sample size is
$n_i$ and the probability of success is simply the accuracy rate $A_i$. Let $c_i$ be
the number of correctly labelled objects belonging to class $i$ in the test set. Then,
conditional on the accuracy rate, $c_i$ follows a binomial distribution:
	\begin{IEEEeqnarray}{lCl}
		(c_i \mid A_i) &\sim& \Bin(n_i, A_i) \label{eqn:likelihood}
	\end{IEEEeqnarray}
In the Bayesian setting, \eqref{eqn:prior} is the prior and \eqref{eqn:likelihood}
is the likelihood. To get the posterior PDF, we simply multiply the prior with the likelihood:
	\begin{IEEEeqnarray*}{lCl}
		f_{A_i \mid \bm{c}}(a)
		&\propto& f_{A_i}(a) \times f_{c_i \mid A_i}(c_i) \\
		&\propto& a^{1-1}(1-a)^{1-1} \times a^{c_i} (1 - a)^{n_i - c_i} \\
		&=& a^{1 + c_i - 1}(1-a)^{1 + n_i - c_i - 1}
	\end{IEEEeqnarray*}
Thus, with respect to the binomial likelihood function,
the Beta distribution is conjugate to itself. The posterior accuracy rate $A_i$
also follows a Beta distribution, now with parameters
	\begin{IEEEeqnarray*}{lCl}
		(A_i \mid c_i) &\sim& \Beta(1 + c_i, 1 + n_i - c_i)
	\end{IEEEeqnarray*}
Recall that our goal is to have a balanced accuracy rate, $A$, that puts an equal
weight in each class. This can be achieved by taking the average of all the class accuracy rates:
	\begin{IEEEeqnarray*}{lCl}
		A &=& \frac{1}{k} \sum_{i=1}^k A_i \\
		&=& \frac{1}{k} A_T
	\end{IEEEeqnarray*}
Here we have defined $A_T$ to be the sum of the individual accuracy rates.
We call  $(A \mid \bm{c})$ the posterior balanced accuracy rate, where
$\bm{c} =(c_1,...,c_k)$.
Most of the time, we simply want to calculate its expected value:
	\begin{IEEEeqnarray*}{lCl}
		\E{A \given \bm{c}} &=& \frac{1}{k} \, \E{A_T \given \bm{c}} \\
		&=& \frac{1}{k} \int a \cdot f_{A_T \mid \bm{c}}(a) \, da
	\end{IEEEeqnarray*}
Note that there is no closed form solution for the PDF $f_{A_T \mid \bm{c}}(a)$.
However assuming that $A_T$ is a sum of $k$ independent Beta random variables,
$f_{A_T \mid \bm{c}}(a)$ can be approximated by numerically convolving $k$ Beta distributions.
The independence assumption is reasonable here, since there should be little to no correlation
between the individual class accuracy rates. Knowing that a classifier is really good
at recognising stars does not tell us much about how well that classifier can recognise
galaxies.

Having the knowledge of $f_{A \mid \bm{c}}(a)$ will allow us to make violin plots,
construct confidence intervals and do hypothesis tests. To get an expression for this,
let us first rewrite the CDF as
	\begin{IEEEeqnarray*}{lCl}
		F_{A\mid \bm{c}}(a) &=& \Prob{A \leq a \mid \bm{c}} \\
		&=& \Prob[\Big]{\frac{1}{k} A_T \leq a \given \bm{c}} \\
		&=& \Prob{A_T \leq ka \given \bm{c}} \\
		&=& F_{A_T \mid \bm{c}}(ka) \IEEEyesnumber \label{eqn:CDF}
	\end{IEEEeqnarray*}
Differentiating \eqref{eqn:CDF} with respect to $a$, we obtain the PDF of $(A \mid \bm{c})$:
	\begin{IEEEeqnarray*}{lCl}
		f_{A \mid \bm{c}}(a) &=& \frac{\partial}{\partial a} F_{A \mid \bm{c}}(ka) \\
		&=& \frac{\partial}{\partial a} (ka) \cdot \frac{\partial}{\partial ka} F_{A_T \mid \bm{c}}(ka) \\
		&=& k \cdot f_{A_T \mid \bm{c}}(ka)
	\end{IEEEeqnarray*}

\subsection{Recall}
Let $C$ be the confusion matrix, where entry $C_{ij}$ is the number of objects in class $i$
but have been classified as class $j$.
Recall measures the classifier's ability to find all the positive examples and avoid
having false negatives:
	\begin{IEEEeqnarray*}{lCl}
		\text{Recall}_i &=& \frac{C_{ii}}{\sum_j C_{ij}}
	\end{IEEEeqnarray*}


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
